<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://yyang768osu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yyang768osu.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2022-02-13T23:06:36+00:00</updated><id>https://yyang768osu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Enforcing Lipschitz Constant in Neural Network</title><link href="https://yyang768osu.github.io/blog/2021/enforcing-lipschitz-constant-in-neural-network/" rel="alternate" type="text/html" title="Enforcing Lipschitz Constant in Neural Network" /><published>2021-04-03T00:00:00+00:00</published><updated>2021-04-03T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2021/enforcing-lipschitz-constant-in-neural-network</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2021/enforcing-lipschitz-constant-in-neural-network/"><![CDATA[<p>A function \(g(x)\) is Lipschitz continuous if there exists a constant \(L\) such that \(\|g(x_1) - g(x_2)\| &lt; L \|x_1 - x_2\|\) for any \(x_1\) and \(x_2\) in its domain. \(L\) is referred to as a Lipschitz constant of \(g\). The need to enforce a certain Lipschitz constant of neural networks arises in many cases, with some examples listed below. Here we introduce a common technique used in many existing literatures.</p>

<ul>
  <li>Guarantee invertibility in normalizing flows build with residual blocks
    <ul>
      <li><a href="https://arxiv.org/abs/1811.00995">iResNet(ICML2019)</a></li>
    </ul>
  </li>
  <li>Discriminator regularization in GAN training
    <ul>
      <li><a href="https://arxiv.org/abs/1701.07875">Wasserstein-GAN(ICML2017)</a></li>
      <li><a href="https://arxiv.org/abs/1802.05957">SpectralNormalization(ICLR2018)</a></li>
    </ul>
  </li>
  <li>Improve network robustness against adversarial perturbations
    <ul>
      <li><a href="https://arxiv.org/abs/1802.04034">Lipschitz-margin-training(NIPS2018)</a></li>
    </ul>
  </li>
</ul>

<p>A small note before we proceed: Lipschitz continuous/constant is defined with respect to a choice of the norm \(\|\cdot\|\). Here we focus on 2-norm.</p>

<h2 id="lipschitz-constant-vs-spectral-norm-of-matrices">Lipschitz constant vs spectral norm of matrices</h2>

<p>Deep neural networks are typically build with interleaved linear layers (such as Conv, TConv, Pooling) and nonlinear activations (such as ReLU, sigmoid). The Lipschitz constant of most activation functions are either constant or easy to control, so we will only focus on linear operations. Linear operations in general can be expressed as in the form of matrix-vector product \(y = g(x) = Wx\) where \(W\) denotes a matrix. In this case, the smallest Lipschitz constant of \(g\) can be expressed as</p>

<p>\begin{equation}
\label{eq:lipconst}
\min_{x_1, x_2, x_1\not=x_2} \frac{
||g(x_1)-g(x_2)||
}{
||x_1 - x_2||
}
=
\min_{||v||\not=0}\frac{
||Wv||
}{
||v||
}
=
\min_{||v||=1}
||Wv||.
\end{equation}</p>

<p>The last term is also known as the <em>spectral norm</em> of matrix \(W\). Let us express \(W\) as its singular-value-decomposition \(U\Sigma V^T\), then we can see that the spectral norm of \(W\) is its maximum singular value, denoted as \(\sigma_1\). The maximum singular value of \(W\) is also the maximum eigenvalue of \(M\triangleq W^TW\) given that eigenvalues of \(W^TW\) is square of singular values of \(W\): \(M=V\Sigma U^TU\Sigma V^T=V\Sigma^2V^T=V\Lambda V^T\).</p>

<p>Now we know that obtaining the best Lipschitz constant of a linear operations amounts to finding the dominant singular value of its matrix representation \(W\), or dominant eigenvalue of \(M\triangleq W^TW\). Next let us introduce an iterative algorithm that can find it.</p>

<h2 id="power-method-aka-von-mises-iteration">Power method (aka Von Mises iteration)</h2>

<p>Power method finds the maximum eigenvalue of a matrix \(M\) using the following iteration:</p>

\[\begin{align*}
&amp;\text{start with a random vector }v^{(0)} \\
&amp;\text{for }k=1, 2, \ldots, \\
&amp;v^{(k)} = \frac{
M v^{(k-1)}
}{
||M v^{(k-1)}||
}
\end{align*}\]

<p>Claim: \(\|M v^{(k)}\|\) converges to the maximum eigen-value of \(M\) as \(k\) approaches infinity.</p>

<p>To show it, let us write the initial vector \(v^{(0)}\) as a linear combinations of eigen-vectors of \(M\): \(v^{(0)}=\sum_{i}\alpha_i v_i\), and expand the iterative formula as</p>

\[\begin{align*}
&amp;v^{(k)} = \frac{
M v^{(k-1)}
}{
||M v^{(k-1)}||
}
= \frac{
M^2 v^{(k-2)}
}{
||M^2v^{(k-2)}||
}=\ldots
= \frac{
M^k v^{(0)}
}{
||M^kv^{(0)}||
}
\\
&amp;M^k v^{(0)} = M^k \sum_{i} \alpha_i v_i = \sum_{i} \alpha_i M^k v_i =\sum_{i}\alpha_i \lambda_i^k v_i = \alpha_1\lambda_1^k
\left(
v_1 + \sum_{i&gt;1}\underbrace{\frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_i}{\lambda_1}\right)^k}_{\to 0 \text{ as } k\to\infty} v_i
\right).
\end{align*}\]

<p>From the last equation we know that \(v^{(k)}\) converges to the dominant eigen-vector \(v_1\) of \(M\) up to a sign difference, and similarly \(Mv^{(k)}\) converges to the maximum eigen-value \(\sigma_1\) of \(M\).</p>

\[\begin{align*}
v^{(k)}\to\left\{\begin{array}{ll}
v_1 &amp; \text{if }\alpha_1&gt;0\\
-v_1 &amp; \text{if }\alpha_1&lt;0
\end{array}\right., \text{ as }k\to\infty
\end{align*}\]

<h2 id="compute-power-iteration-through-auto-differentiation">Compute power iteration through auto-differentiation</h2>

<p>From last section we know that the maximum singular value can be computed if we carry out the following iteration procedure:</p>

\[\begin{align*}
 v^{(k-1)} \Longrightarrow \underbrace{\tilde{v}^{(k)}=W^TWv^{(k-1)}}_{\text{step 1}} \Longrightarrow \underbrace{v^{(k)}=\tilde{v}^{(k)}/||\tilde{v}^{(k)}||}_{\text{step 2}} \Longrightarrow\ldots
\end{align*}\]

<p>While it is easy to compute vector norm as done in step 2, it is not immediately clear how to easily compute \(W^TWv^{(k-1)}\) in step 1, since expressing \(W\) explicitly for a general linear layer can be involved. For instance, for a 2D convolution operation, expressing it in the matrix-vector product form requires unpacking the convolution kernel into a doubly Toeplitz matrix. We know that \(Wv^{(k-1)}\) is just the output of the linear operator \(g\) when \(v^{(k-1)}\) is used as input, but seemingly there is no easy way to multiply by \(W^T\) without knowing \(W\) explicitly.</p>

<p>Here’s the trick: we can express \(W^TWx\) as the derivative of another another function and compute it with auto-differentiation.</p>

\[\begin{align*}
W^TW x = \frac{1}{2}\frac{\partial x^TW^TWx}{\partial x} = \frac{1}{2}\frac{\partial ||Wx||^2}{\partial x} =\frac{\partial \frac{1}{2}||g(x)||^2}{\partial x}
\end{align*}\]

<p>We can then modify the iteration procedure as</p>

\[\begin{align*}
 v^{(k-1)} \Longrightarrow \underbrace{
\tilde{v}^{(k)} = \frac{
\partial\frac{1}{2}||g(v^{v^{(k-1)}})||^2
}{
\partial v^{(k-1)}
}
 }_{\text{step 1}} \Longrightarrow \underbrace{v^{(k)}=\tilde{v}^{(k)}/||\tilde{v}^{(k)}||}_{\text{step 2}} \Longrightarrow\ldots
\end{align*}\]

<p>Based on last section, \(\sqrt{\|\tilde{v}^{(k)}\|}\) yields an estimate of the dominant singular value of \(M\), which is the Lipschitz constant of the linear operator \(g\).
In PyTorch, step 1 can be calculated using <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad">torch.atuograd.grad</a>.</p>

<p>It should not be surprising that the above iteration procedure converges to maximum singular value of \(W\) – it is simply the gradient ascent with Equation \eqref{eq:lipconst} as the optimization objective.</p>

<h2 id="enforce-lipschitz-constant-c-during-training">Enforce Lipschitz constant \(c\) during training</h2>

<p>It is easy to see that the Lipschitz constant of \(a\times g(\cdot)\) is \(a\) times the Lipschitz constant of \(g(\cdot)\), or more precisely, \(\text{Lip}(ag) = a\text{Lip}(g)\). To enforce the Lipschitz constant of an operator to be some target value \(c\), we just need to normalize the output the operator by \(c/\text{Lip}(g)\).</p>

<p>The power iteration procedure itself can be amortized and blended into the optimization step of the network training, in which case the training loop can be expressed as</p>

\[\begin{align*}
&amp;\text{for step }k=1, \ldots:\\
&amp;v = v/||v||\\
&amp;v = \frac{
\partial\frac{1}{2}||g(v)||^2
}{
\partial v 
}\\
&amp;\sigma = \sqrt{||v||}\\
&amp;\text{set the normalization scale of output of }g\text{ as }\frac{c}{\sigma}\\
&amp;\text{the rest of the training step}.
\end{align*}\]]]></content><author><name></name></author><summary type="html"><![CDATA[how to enforce lipschitz constraint in neural networks?]]></summary></entry><entry><title type="html">Langevin Dynamics for Bayesian Inference</title><link href="https://yyang768osu.github.io/blog/2020/langevin-dynamics/" rel="alternate" type="text/html" title="Langevin Dynamics for Bayesian Inference" /><published>2020-09-06T00:00:00+00:00</published><updated>2020-09-06T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2020/langevin-dynamics</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2020/langevin-dynamics/"><![CDATA[<p>In this post we visit some technical details centered around Langevin Dynamics in the context of stochastic Bayesian learning, assuming minimal background on conventional calculus and Brownian motion. Starting with quadratic variation, we gradually show how Ito’s Lemma and Fokker-Planck equation can be derived. Using Fokker-Planck equation, it is revealed that an Langevian dynamic can be used as a MCMC method to generate samples from an un-normalized distribution. Lastly, <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">stochastic gradient Langevin dynamics</a> method is discussed.</p>

<p>The following materials are taken as references:</p>

<ul>
  <li><a href="https://www.math.ucdavis.edu/~hunter/m280_09/ch5.pdf">UC-Davis Lecture Notes on Applied Mathematics</a></li>
  <li><a href="https://www.youtube.com/watch?v=PPl-7_RL0Ko">MIT Topics in Mathematics with Applications in Finance Lecture 17: Stochastic Processes II</a></li>
  <li><a href="https://www.youtube.com/watch?v=Z5yRMMVUC5w">MIT Topics in Mathematics with Applications in Finance Lecture 18: Itō Calculus</a></li>
</ul>

<h2 id="quadratic-variation">Quadratic Variation</h2>
<p>For Brownian motion \(B_t\), 
we know that \(B\left(\frac{i+1}{N}T\right) - B\left(\frac{i}{N}T\right)\) for different index \(i\) are i.i.d. with distribution \(\mathcal{N}\left(0, \frac{T}{N}\right)\). The following holds by strong law of large numbers.</p>

\[\begin{align*}
\lim_{N\to\infty}\sum_{i=1}^N \left(B\left(\frac{i+1}{N}T\right) - B\left(\frac{i}{N}T\right)\right)^2&amp;=T \text{ a.s.} \\
\end{align*}\]

<p>The above can be written in differential form as</p>

\[\begin{align*}
\int (dB)^2 &amp;= \int dt\\
(dB)^2 &amp;= dt\\
\end{align*}\]

<p>which is known as quadratic variation. This means that the second order term of Taylor expansion involving \(B_t\) scales as \(O(t)\) instead of \(o(t)\), the implication of which is detailed in Ito’s Lemma below.</p>

<h2 id="itos-lemma">Ito’s Lemma</h2>

<p>Suppose we want to compute \(f(B_t)\) for some smooth function \(f\). By Taylor expansion, the infinitesimal difference can be expressed as</p>

\[\begin{align*}
f(B_{t+\Delta t}) - f(B_t) &amp;= f'(B_t) (B_{t+\Delta t} - B_t) + \frac{f''(B_t)}{2}\left(B_{t+\Delta t}-B_t\right)^2 \\
                  \text{(differential form) }      df &amp;= f'(B_t) dB_t + \frac{f''(B_t)}{2}\left(dB_t\right)^2 \\
                  \text{(quadratic variation) }      df &amp;= f'(B_t) dB_t + \frac{f''(B_t)}{2} dt\\
                       \frac{df}{dt} &amp;= f'(B_t) \frac{dB_t}{dt} \color{red}{+ \frac{f''(B_t)}{2}}
\end{align*}\]

<p>The above equation is a naive version of Ito’s Lemma, the basis of Ito’s calculus. Note how it differs from conventional calculus by having the second term in red, as a direct consequence of quadratic variation.</p>

<p>Let us now look at a more advanced version of Ito’s Lemma, with the goal of obtaining the differential form of \(f(x_t, t)\) where \(x_t\) is a stochastic process defined with the following stochastic differential equation</p>

\[\begin{align*}
dx_t = \mu(x_t)dt + \sigma dB_t
\end{align*}\]

<p>Similarly as before, let’s apply Taylor expansion on the infinitesimal difference of \(f\)</p>

\[\begin{align*}
f(x+\Delta x, t+\Delta t) - f(x, t) &amp;= \frac{\partial f}{\partial t} \Delta t + \frac{\partial f}{\partial x} \Delta x + \frac{1}{2}\left[
\frac{\partial^2 f}{\partial t^2}\Delta t^2 + 2\frac{\partial^2 f}{\partial t \partial x} \Delta t \Delta x + \frac{\partial^2 f}{\partial x^2}(\Delta x)^2
\right] \\
\text{(differential form) } d f &amp;= \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial x} dx_t +\frac{1}{2}\left[
o(dt) + o(dt) + \frac{\partial^2 f}{\partial x^2}(dx_t)^2
\right] \\
\text{(substitute $dx_t$) } d f &amp;= \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial x} (\mu(x_t)dt +\sigma dB_t) +\frac{1}{2}
\frac{\partial^2 f}{\partial x^2}(\mu(x_t)^2(dt)^2 + 2\mu(x_t)\sigma dt dB_t + \sigma^2 (dB_t)^2)
\\
\text{(quadratic variation) } d f &amp;= \left(\frac{\partial f}{\partial t} + \mu(x_t)\frac{\partial f}{\partial x} + \color{red}{\frac{1}{2}\sigma^2\frac{\partial^2 f}{\partial x^2}}\right)dt + \sigma\frac{\partial f}{\partial x} dB_t
\end{align*}\]

<p>Again, the red term highlights the difference to conventional calculus. In the special when \(f\) is not a function of \(t\), the above can be reduced to</p>

\[\begin{align*}
 d f &amp;= \left(\mu(x_t)f'(x) + \color{red}{\frac{1}{2}\sigma^2 f''(x)}\right)dt + \sigma f'(x) dB_t,
\end{align*}\]

<p>which is used in the derivation of Fokker-Planck equation in the next section.</p>

<h2 id="fokker-planck-equation">Fokker-Planck equation</h2>

<p>For a stochastic process \(x\) that is defined as \(dx_t = \mu(x_t) dt + \sigma dB_t\), we are interested in how the distribution \(p_t\) of \(x_t\) evolves over time. For an arbitrary smooth function \(f\), the following holds</p>

\[\begin{align*}
\frac{d}{dt}\mathbb{E}\left[f(x_t)\right] = \left\{
\begin{array}{ll}
\int f(x) \frac{d}{dt}p_t(x) dx &amp; \text{ $f(x)$ viewed as a function of $x$ sampled from $p_t$} \\
\mathbb{E}\left[\frac{d}{dt}f(x_t)\right] &amp; \text{ $f(x_t)$ viewed as a function of stochastic process $x_t$}\\
\end{array}
\right.
\end{align*}\]

<p>The second expression can be evaluated with Ito’s Lemma.</p>

\[\begin{align*}
&amp;\mathbb{E}\left[\frac{d}{dt}f(x_t)\right]\\
\text{(Ito's Lemma) }=&amp;\mathbb{E}\left[\mu(x_t)f'(x_t)+\frac{1}{2}\sigma^2f''(x_t) + \sigma f'(x_t) \frac{dB_t}{dt}\right] \\
\text{($dB_t$ has mean $0$) }=&amp;\mathbb{E}\left[\mu(x_t)f'(x_t)+\frac{1}{2}\sigma^2f''(x_t) \right] \\
\text{(using $x_t\sim p_t$) }=&amp;\int\left[\mu(x)f'(x)+\frac{1}{2}\sigma^2f''(x) \right]p_t(x)dx \\
\text{(integration by part) }=&amp;-\int f(x)\frac{\partial (\mu(x)p_t(x)) }{\partial x}dx+\frac{1}{2}\sigma^2\int f(x)\frac{\partial^2 p_t(x)}{\partial x^2} p_t(x)dx
\end{align*}\]

<p>Combining the above two and cancelling out the arbitrary function \(f\), we obtain Fokker-Planck equation below.</p>

\[\begin{align*}
\frac{d}{dt}p_t = -\frac{\partial}{\partial x}\left(\mu(x)p_t(x)\right)+\frac{1}{2}\sigma^2\frac{\partial^2}{\partial x^2}p_t(x)
\end{align*}\]

<h2 id="langevin-dynamics">Langevin Dynamics</h2>

<p>let \(\mu(x) = -u'(x)\) for some function \(u(x)\), then the corresponding stochastic process is defined as \(dx_t = -u'(x_t) dt + \sigma dB_t\), often referred as over-damped Langevin process. Using Fokker-Planck equation, we know that</p>

\[\begin{align*}
p(x) \propto e^{-2/\sigma^2 u(x)}
\end{align*}\]

<p>is the stationary distribution of \(x_t\).</p>

\[\begin{align*}
&amp;\frac{\partial}{\partial x}\left(u'(x)p(x)\right)+\frac{1}{2}\sigma^2\frac{\partial^2}{\partial x^2}p(x) \\
=&amp;u''(x)p(x)-\frac{2}{\sigma^2}(u'(x))^2 p(x) + \frac{1}{2}\sigma^2\left(-\frac{2}{\sigma^2}u''(x)p(x) + \frac{4}{\sigma^4}(u'(x))^2 p(x)\right)=0
\end{align*}\]

<h3 id="langevin-mcmc">Langevin MCMC</h3>

<p>The fact that Langevin process \(dx_t = -u'(x_t) dt + \sigma dB_t\) converges to a stationary distribution \(p(x) \propto e^{-2/\sigma^2 u(x)}\) lends itself as a suitable Markov chain Monte Carlo method. Specifically, to obtain samples from a un-normalized density function \(\bar{p}(x)\), we just need to run the following Langevin process from a random starting point till it reaches steady state distribution</p>

\[\begin{align*}
dx_t = \nabla_x \log \bar{p}(x) dt + \sqrt{2} dB_t
\end{align*}\]

<p>Discretized sample path of Langevin process can be generated with Euler method</p>

\[\begin{align*}
x_{k+1}  = x_k  + \nabla_x \log \bar{p}(x_k) \epsilon + \sqrt{2\epsilon}\xi_k
\end{align*}\]

<p>Since the discretization is only an approximation to the original continuous stochastic process, it does not in itself lead to desired stationary distribution (unless \(\epsilon\) becomes infinitesimal) and thus should be corrected by Metropolis-Hastings to enforce detailed balance condition.</p>

<p>One lingering question is: does the discretization of Langevin dynamics satisfy detailed balance equation in \(\epsilon\to0\) asymptote? The fact that it converges to a desirable distribution does not indicate that it is a time-reversible Markov chain. Even thought it is claimed by some source that the asymptotic acceptance ratio approaches 1, I was not able to show that it is the case and is stuck at the following derivation.</p>

\[\begin{align*}
&amp;\frac{\bar{p}(x)P(x\to x')}{\bar{p}(x')P(x'\to x)} = \frac{
\bar{p}(x)\mathcal{N}\left(x'-x-\nabla_x \bar{p}(x)\tau|0, 2\tau\right)
}{
\bar{p}(x')\mathcal{N}\left(x-x'-\nabla_x \bar{p}(x')\tau|0, 2\tau\right)
}\\
=&amp; 
\frac{
\bar{p}(x)e^{(x'-x)\nabla_x \bar{p}(x)/2 + o(\tau)}
}{
\bar{p}(x')e^{(x-x')\nabla_x \bar{p}(x')/2 + o(\tau)} 
}
=
\frac{
\bar{p}(x)e^{(x'-x)\nabla_x \frac{\bar{p}(x)+\bar{p}(x')}{2} + o(\tau)}
}{
\bar{p}(x') 
}
\end{align*}\]

<h3 id="relevance-to-bayesian-inference">Relevance to Bayesian Inference</h3>

<p>In Bayesian inference we deal with a prior distribution \(p_\text{prior}(\theta)\) for some latent parameter \(\theta\) and a likelihood term \(p_\text{likelihood}(\mathcal{D}\|\theta)\) of the dataset \(\mathcal{D}\) given the latent parameter, and the goal is to obtain samples according to the posterior probability \(p_\text{post}(\theta\|\mathcal{D}) = p_\text{prior}(\theta)p_\text{likelihood}(\mathcal{D}\|\theta)/p(\mathcal{D})\). Since the constant marginal likelihood term \(p(\mathcal{D})=\int p_\text{likelihood}(\mathcal{D}\|\theta)p_\text{prior}(\theta)d\theta\) is often intractable, we are left with a un-normalized poster probability \(p_\text{post}\propto p_\text{prior}p_\text{likelihood}\). To sample from it, we can simply construct and run the following stochastic process</p>

\[\begin{align*}
d\theta_t = \left(\nabla_\theta \log p_\text{prior}(\theta) + \nabla_\theta \log p_\text{likelihood}(\mathcal{D}|\theta)\right) dt + \sqrt{2} dB_t
\end{align*}\]

<p>Hereafter we use the notation of \(x\) to indicate elements in the dataset \(x\in\mathcal{D}\), \(\theta\) to denote the hidden parameter for which we want to conduct Bayesian inference, and drop the subscript to different \(p\) as they can be differentiated by their arguments.</p>

<h2 id="stochastic-gradient-langevin-dynamics-sgld">Stochastic Gradient Langevin Dynamics (SGLD)</h2>

<p>Discretizing Langevin dynamics with step size of \(\epsilon_t\) leads to the following update rule</p>

\[\begin{align*}
\Delta \theta = \epsilon_t \left(\nabla_\theta \log p(\theta) + \nabla_\theta \log p(\mathcal{D}|\theta)\right) + \sqrt{2 \epsilon_t} \xi_t, \text{ where }\xi_t\sim\mathcal{N}(0, 1)
\end{align*}\]

<p>If we have \(\sum_t\epsilon_t = \infty\) and \(\sum_t\epsilon^2 &lt;\infty\) then asymptotically the discretization error will become negligible and the update rule approaches the corresponding Langevin dyanmics, resulting in a sequence of \(\theta_t\) that converges to the posterior distribution \(p(\theta\|\mathcal{D})\).</p>

<p>An interesting and clever observation made by <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">stochastic gradient Langevin dynamics</a> paper is that the convergence will hold even if we use mini-batches of the data to estimate the gradient of \(\nabla_\theta \log p(\mathcal{D}\|\theta)\).</p>

\[\begin{align*}
\nabla_\theta \log p(\mathcal{D}|\theta) \approx \frac{N}{n}\sum_{i=1}^n\nabla_\theta \log p(x_{t,i}|\theta)
\end{align*}\]

<p>The insight is that the stochastic error introduced from using mini-batches instead of the whole dataset dies out much faster than the added Gaussian noise as the \(\epsilon_t\) decreases, so it does not change the asymptotical behavior of the update rule. Specifically, the randomness coming from the stochastic estimate of \(\nabla_\theta \log p(\mathcal{D}\|\theta)\) has a variance that scales as \(\epsilon_t^2\) since it is multiplied with \(\epsilon_t\). In comparison, the variance of the added Gaussian noise scales linearly as \(\epsilon_t\).</p>

\[\begin{align*}
\Delta \theta =\underbrace{ \underbrace{ \underbrace{\epsilon_t \frac{N}{n}\sum_{i=1}^n\nabla_\theta \log p(x_{t, i}|\theta)}_{\text{gradient step towards ML target}} +\epsilon_t \nabla_\theta \log p(\theta)}_{\text{gradient step towards MAP target}} + \sqrt{2 \epsilon_t} \xi_t}_{\text{stochastic gradient Langevin dynamics for posterior sampling}} , \text{ where }\xi_t\sim\mathcal{N}(0, 1)
\end{align*}\]

<p>Given that stochastic Langevin dynamics converges to the desired distribution as \(\epsilon_t\to0\), we do not need to carry out Metropolis-Hastings to reject samples. This is crucial in simplifying the algorithm, since evaluation of rejection/acceptance rate is computed at every step and it depends on the evaluation of \(p(\theta)p(\mathcal{D}\|\theta)\) which can only be computed after traversing the whole dataset.</p>

<p>As a closing remark, if we use the posterior sampling for the estimation of the expectation of some function \(f\), it is recommended in <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">stochastic gradient Langevin dynamics</a> that the following equation be used.</p>

\[\begin{align*}
\mathbb{E}[f(\theta)] = \frac{\sum_t \epsilon_t f(\theta_t)}{\sum_t \epsilon_t}
\end{align*}\]

<p>with the intuition that each \(\theta_t\) will contribute an effective sample size proportional to \(\epsilon_t\).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[stochastic differential equation, Fokker Plank equation, and their connections to Bayesian inference]]></summary></entry><entry><title type="html">Understanding and Implementing Asymmetric Numeral System (ANS)</title><link href="https://yyang768osu.github.io/blog/2020/understanding-and-implementing-ans/" rel="alternate" type="text/html" title="Understanding and Implementing Asymmetric Numeral System (ANS)" /><published>2020-06-26T00:00:00+00:00</published><updated>2020-06-26T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2020/understanding-and-implementing-ans</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2020/understanding-and-implementing-ans/"><![CDATA[<p>Denote an alphabet with \(N\) different symbols as \(\mathcal{A}=\{0, 1, \ldots, N-1\}\). Let us consider a source coding algorithm where a sequence of these symbols are encoded into a sequence of bits, which is represented by an integer \(s\), and assume that we can decode each symbol sequentially by breaking down \(s\) into one symbol \(x\in\mathcal{A}\) and a new integer \(s'\in\mathbb{N}\) capturing information of the remaining symbols. The encoding and decoding operation can be represented by the following <code class="language-plaintext highlighter-rouge">push</code> and <code class="language-plaintext highlighter-rouge">pop</code> operation:</p>

\[\begin{align*}
\text{encode/push  }e:&amp; \mathbb{N}\times\mathcal{A} \to \mathbb{N}\\
\text{decode/pop   }d:&amp; \mathbb{N} \to \mathbb{N}\times\mathcal{A}
\end{align*}\]

<p>There are two design goals for such a codec:</p>
<ol>
  <li>validity: for valid encoding and decoding, we want to make sure that \(e\) and \(d\) are bijections and inverse of each other (\(e=d^{-1}\)).</li>
  <li>efficiency: for coding efficiency, we want the final codeword length to approach the entropy of the data source</li>
</ol>

<p>Let us focus on the decoding process \(d\) and denote the mapping from an integer \(s\) using \(d\) as \(d(s) = s', x\) where \(x\in\mathcal{A}\). From information theory we know that, the information content (the amount of surprise) of the event of encountering \(x\) with probability \(P(x)\) can be expressed as \(-\log P(x)\). Then, to optimal performance we expect that the codeword length used to represent \(x\) to be roughly \(1/\log(P(x))\). In other words, in an efficient encoding algorithm, the number of bits in \(s\) (\(\log(s)\)) to be \(1/\log(P(x))\) more than that of \(s'\):</p>

\[\begin{align*}
\log(s) - \log(s') \approx \frac{1}{\log P(x)}\text{ for }d(s) = x, s'.
\end{align*}\]

<p>Expressed in another way</p>

\[\begin{align*}
\frac{s'}{s} = P(x)\text{ for }d(s) = x, s'.
\end{align*}\]

<p>The question is: how can we design a bijective mapping from \(\mathbb{N}\) and \(\mathbb{N}\times\mathcal{A}\) satisfying the above goal? Here’s the core idea of asymmetric numerical system: let us assume that we have access to a function that maps each of the number in \(\mathbb{N}\) into one of the symbols in \(\mathcal{A}\), denoted as \(h:\mathbb{N}\to\mathcal{A}\) with the property that for any integer \(s\) and symbol \(x\), there are roughly \(P(x)\times s\) numbers below \(s\) with symbol \(x\). Put more precisely,</p>

\[\begin{align*}
\frac{|\{n\in\mathbb{N}, n&lt;s, h(n) = x\}|}{s} \approx P(x) \text{ for any }s\in\mathbb{N}\text{ and }x\in\mathcal{A}.
\end{align*}\]

<p>With such a mapping \(h\) available, we can define the bijective decoder mapping \(d\) to be</p>

\[\begin{align*}
d(s)=&amp;s',x\text{ where} \\
s'=&amp;\left|\left\{n\in\mathbb{N},n&lt;s,h(n)=h(s)\right\}\right|,\\
x=&amp;h(s).
\end{align*}\]

<p>and it is easy to check that our two design goals are satisfied, and now we have shifted our task to finding such a labeling function \(h\) such that it leads to easy computation of \(d\) and \(e\).</p>

<h2 id="mapping-of-natural-numbers-to-symbols-h">Mapping of natural numbers to symbols (\(h\))</h2>

<p>In r-ANS (range-ANS) design, the pmf \(P:\mathcal{A}\to[0, 1]\) is quantized into integers \(p(x)\) where</p>

\[\begin{align*}
&amp;\sum_{x\in\mathcal{A}}p(x)=2^r\\
&amp;p(x)/2^r\approx P(x). 
\end{align*}\]

<p>The mapping \(h\) is design as the following: we divide the natural numbers into chunks with length \(2^r\). Within each chunk, start with symbol \(0\in\mathcal{A}\), we map the first \(p(0)\) numbers to \(0\); then the subsequent \(p(1)\) numbers are mapped to \(1\), so on and so forth.</p>

<p>Let us define \(c:\mathcal{A} \to \mathcal{N}\) with \(c(x)=\sum_{a\in\mathcal{A}, a&lt;x}p(a)\). Then the number from \(c(x)\) to \(c(x)+p(x)\) within a length \(2^r\) chunk is labeled as \(x\).</p>

<p>With this mapping, can express \(d\) and \(e\) into the following arithmetics that are easy to compute:</p>

\[\begin{align*}
d(s) =&amp; p(x)\times(s//2^r) + (s\text{ mod }2^r-c(x)), x\triangleq h(s) \\
e(s', x) =&amp; 2^r \times (s'//p(x)) + (s'\text{ mod }p(x) + c(x))
\end{align*}\]

<p>with this comes the first implementation of ANS</p>

<h2 id="ans-without-rescaling-flawed-version">ANS without rescaling (flawed version)</h2>

<h3 id="encoder">Encoder</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">s</span>
</code></pre></div></div>

<h3 id="decoder">Decoder</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span>
        <span class="c1"># this loop can be improved by binary search
</span>        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">))):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">c</span><span class="p">[</span><span class="n">a</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">a</span>
    <span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="n">s</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">)</span>
</code></pre></div></div>

<p>Running this encoder decoder pair through tests, one will realize that there is a small issue with the encoder and decoder function \(e\) and \(d\). Specifically, if both \(s'\) and \(x\) are \(0\), then \(s=e(0, 0)=0\). This means that any front-loaded \(0\)-sequence will be just be coded into \(0\), and decoder has no ways of knowing how many \(0\) symbols are there in the front of the sequence, if any! To solve this issue, we need to additionally guarantee that \(e\) results in strictly increasing integer. A fixed version is provided in the next section.</p>

<h2 id="ans-without-rescaling-correct-version">ANS without rescaling (correct version)</h2>

<h3 id="encoder-1">Encoder</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="s">""" ANS encoder (no rescaling)

    Parameters
    ----------
    symbols : list of int
        list of input symbols represented by index
        value should not be larger than len(p)
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r

    Warnings
    --------
    int type for all the input arguments should be python int type,
    which has arbitrary precision

    Returns
    -------
    s : integer representation of the encoded message

    """</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">s</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">s</span>
</code></pre></div></div>

<h3 id="decoder-1">Decoder</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="s">""" ANS encoder (no rescaling)

    Parameters
    ----------
    s : int
        integer representation of the encoded message
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r

    Warnings
    --------
    int type for all the input arguments should be python int type,
    which has arbitrary precision

    Returns
    -------
    decoded_symbols : list of int
        list of decoded symbols

    """</span>

    <span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span>
        <span class="c1"># this loop can be improved by binary search
</span>        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">))):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">c</span><span class="p">[</span><span class="n">a</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">a</span>

    <span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="n">s</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">s</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">))</span>
</code></pre></div></div>
<h3 id="test">Test</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># initialize data distribution and input length
</span><span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">106</span><span class="p">]</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">150</span><span class="p">]</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># randomly sample input
</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">symbols</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span> <span class="n">weights</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>

<span class="c1"># encode
</span><span class="n">s</span> <span class="o">=</span> <span class="n">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

<span class="c1"># decode
</span><span class="n">decoded_symbols</span> <span class="o">=</span> <span class="n">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

<span class="c1"># statistics
</span><span class="n">average_bps</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">sequence_length</span>
<span class="n">entropy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="n">i</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># sanity check
</span><span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">,</span> <span class="n">symbols</span><span class="p">))</span>

<span class="c1"># display results
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"encoded integer        : </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"average bits per symbol: </span><span class="si">{</span><span class="n">average_bps</span><span class="p">:.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> bits/symbol"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"data source entropy    : </span><span class="si">{</span><span class="n">entropy</span><span class="p">:.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> bits/symbol"</span><span class="p">)</span>
</code></pre></div></div>

<p>Test output</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encoded</span> <span class="n">integer</span>        <span class="p">:</span> <span class="mi">125621967822099623663819958660494947377946858741001513589</span>
<span class="n">average</span> <span class="n">bits</span> <span class="n">per</span> <span class="n">symbol</span><span class="p">:</span> <span class="mf">1.86357</span> <span class="n">bits</span><span class="o">/</span><span class="n">symbol</span>
<span class="n">data</span> <span class="n">source</span> <span class="n">entropy</span>    <span class="p">:</span> <span class="mf">1.79865</span> <span class="n">bits</span><span class="o">/</span><span class="n">symbol</span>
</code></pre></div></div>

<h2 id="ans-with-rescaling">ANS with rescaling</h2>

<p>The above ANS implementation takes advantage of the fact that python integer has arbitrary precision, which allows us to encode a sequence that is arbitrarily long without overflowing. This poses a complexity issue: the encoding operation gets increasingly hard to compute as integer \(s\) gets larger. Without resolving this reliance on infinite precision integer arithmetic, we cannot implement it using lower-level language with more hardware friendly instructions.</p>

<p>One idea is to limit the range of \(s\), say with a maximum bit-width of \(r_s\). Since the encoding process will necessary increase the value of \(s\), we then need to scale down its value before additional encoding, to a point where we can avoid overflow. In other words, before carrying out the encoding operation os \(e(s', x)\), \(s'\) should satisfy</p>

\[\begin{align*}
&amp;&amp;2^r\times (s'//p(x)) + (s'\text{ mod }p(x) + c(x)) &amp;&lt; 2^{r_s}\\
\Longleftrightarrow&amp;&amp; s'//p(x) + \underbrace{(s'\text{ mod }p(x) + c(x)) / 2^r}_{&lt;1}&amp;&lt; 2^{r_s-r}\\
\Longleftrightarrow&amp;&amp; s'//p(x) &amp;&lt; 2^{r_s-r}\\
\Longleftrightarrow&amp;&amp; s' &amp;&lt; (2^{r_s-r}+1)\times p(x)
\end{align*}\]

<p>In the rescaling implementation, scaling down is achieved by extracting \(r_t\) least significant bits, packing these bits into an integer \(t\), and saving this integer to a stack \(t_\text{stack}\), achieved through the following logic</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">r_s</span> <span class="o">-</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]:</span>
    <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r_t</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="n">r_t</span>
    <span class="n">t_stack</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we have guaranteed that encoder output will be an integer \(s&lt;2^{r_s}\) accompanied by a stack of integers \(t&lt;2^{r_t}\), the next question is, how to perform decoding? An easy answer would be to do the exact inverse of encoding, but to do that we need to know exactly when to perform up-scaling. The key is to realize that after the above loop is performed, it is guaranteed that \(e(s', x)\) is always larger than or equal to \(2^{r_s-r_t}\) (assuming \(r_t &gt; r\)), and thus during decoding we just need to upscale \(s\) whenever it falls below \(2^{r_s-r_t}\).</p>

<p>After the while loop terminates, we have</p>

\[\begin{align*}
&amp;&amp; 2^{r_t} s' + (2^{r_t}-1) &amp;\geq (2^{r_s-r}+1) p(x)\\
\Longleftrightarrow &amp;&amp; 2^{r_t}s' + 2^{r_t} &amp;&gt; (2^{r_s-r}+1)p(x)\\
\Longleftrightarrow &amp;&amp; 2^{r_t}s'&amp;&gt; 2^{r_s-r}p(x) + p(x) - 2^{r_t}\\
\Longleftrightarrow &amp;&amp;        s'&amp;&gt; 2^{r_s-r-r_t}p(x) + \underbrace{p(x)/2^{r_t}}_{&lt;1} - 1\\
\Longleftrightarrow &amp;&amp;        s'&amp;\geq 2^{r_s-r-r_t}p(x)
\end{align*}\]

<p>Plugging in the above inequality to \(e(s', x)\), we have</p>

\[\begin{align*}
e(s', x) \geq 2^{r_s -r_t}.
\end{align*}\]

<p>Now we are ready to implement ANS with rescaling.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_s</span><span class="p">,</span> <span class="n">r_t</span><span class="p">):</span>
    <span class="s">""" ANS encoder

    Parameters
    ----------
    symbols : list of int
        list of input symbols represented by index
        value should not be larger than len(p)
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r
    r_s : int
        bit-width precision of encoded integer s
    r_t : int
        bit-width precision of integers in stack t_stack

    Returns
    -------
    s : int
        s &lt; 2 ** r_s
    t_stack : list of int
        each int &lt; 2 ** r_t

    """</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">t_stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">r_s</span> <span class="o">-</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]:</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">s</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">r_t</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="n">r_t</span>
            <span class="n">t_stack</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">t_stack</span>


<span class="k">def</span> <span class="nf">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t_stack</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_s</span><span class="p">,</span> <span class="n">r_t</span><span class="p">):</span>
    <span class="s">""" ANS encoder

    Parameters
    ----------
    s : int
        (s, t_stack) together represent the encoded message; s &lt; 2 ** r_s
    t_stack : list of int
        (s, t_stack) together represent the encoded message; t &lt; 2 ** r_t
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r
    r_s : int
        bit-width precision of encoded integer s
    r_t : int
        bit-width precision of integers in stack t_stack

    Returns
    -------
    decoded_symbols : list of int
        list of decoded symbols

    """</span>

    <span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span>
        <span class="c1"># this loop can be improved by binary search
</span>        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">))):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">c</span><span class="p">[</span><span class="n">a</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">a</span>

    <span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="n">s</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">s</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">r_s</span> <span class="o">-</span> <span class="n">r_t</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_stack</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">t_stack</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span> <span class="o">&lt;&lt;</span> <span class="n">r_t</span><span class="p">)</span> <span class="o">+</span> <span class="n">t</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">))</span>


<span class="c1">## Test code
</span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># initialize data distribution and input length
</span><span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">106</span><span class="p">]</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">150</span><span class="p">]</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">r_s</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">r_t</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># randomly sample input
</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">symbols</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span> <span class="n">weights</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>

<span class="c1"># encode
</span><span class="n">s</span><span class="p">,</span> <span class="n">t_stack</span> <span class="o">=</span> <span class="n">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_s</span><span class="p">,</span> <span class="n">r_t</span><span class="p">)</span>

<span class="c1"># decode
</span><span class="n">decoded_symbols</span> <span class="o">=</span> <span class="n">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t_stack</span><span class="p">[:],</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_s</span><span class="p">,</span> <span class="n">r_t</span><span class="p">)</span>

<span class="c1"># statistics
</span><span class="n">average_bps</span> <span class="o">=</span> <span class="p">(</span><span class="n">r_s</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_stack</span><span class="p">)</span> <span class="o">*</span> <span class="n">r_t</span><span class="p">)</span> <span class="o">/</span> <span class="n">sequence_length</span>
<span class="n">entropy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="n">i</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># sanity check
</span><span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">,</span> <span class="n">symbols</span><span class="p">))</span>

<span class="c1"># display results
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"average bits per symbol: </span><span class="si">{</span><span class="n">average_bps</span><span class="p">:.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> bits/symbol"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"data source entropy    : </span><span class="si">{</span><span class="n">entropy</span><span class="p">:.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> bits/symbol"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="test-results">Test results</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">average</span> <span class="n">bits</span> <span class="n">per</span> <span class="n">symbol</span><span class="p">:</span> <span class="mf">1.80960</span> <span class="n">bits</span><span class="o">/</span><span class="n">symbol</span>
<span class="n">data</span> <span class="n">source</span> <span class="n">entropy</span>    <span class="p">:</span> <span class="mf">1.79865</span> <span class="n">bits</span><span class="o">/</span><span class="n">symbol</span>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[an introduction of ANS and its implementation]]></summary></entry><entry><title type="html">Arithmetic Coding (AC) Implementation</title><link href="https://yyang768osu.github.io/blog/2020/arithmetic-coding-implementation/" rel="alternate" type="text/html" title="Arithmetic Coding (AC) Implementation" /><published>2020-06-24T00:00:00+00:00</published><updated>2020-06-24T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2020/arithmetic-coding-implementation</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2020/arithmetic-coding-implementation/"><![CDATA[<h2 id="arithmetic-encoder-infinite-precision">Arithmetic Encoder (infinite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">a</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">symbol</span><span class="p">],</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span>
<span class="k">while</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># case 0
</span>        <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="n">b</span> <span class="o">*=</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># case 1
</span>        <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># a &lt; 1/2 and b &gt; 1/2
</span><span class="k">while</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">4</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># a &lt;= 1/4 or b &gt;= 3/4
</span><span class="k">if</span> <span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">:</span>  <span class="c1"># case 2a
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
<span class="k">else</span><span class="p">:</span>  <span class="c1"># case 2b
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
</code></pre></div></div>

<h2 id="arithmetic-decoder-infinite-precision">Arithmetic Decoder (infinite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">z</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="k">for</span> <span class="n">bit_index</span><span class="p">,</span> <span class="n">bit</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
    <span class="n">binary_block_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="n">bit_index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bit</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">+=</span> <span class="n">binary_block_size</span>
    <span class="n">symbol</span> <span class="o">=</span> <span class="n">decode_one_symbol</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span> <span class="o">+</span> <span class="n">binary_block_size</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">symbol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">symbol</span><span class="p">)</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">symbol</span><span class="p">],</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span>
        <span class="n">symbol</span> <span class="o">=</span> <span class="n">decode_one_symbol</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span> <span class="o">+</span> <span class="n">binary_block_size</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">decode_one_symbol</span><span class="p">(</span><span class="n">z_0</span><span class="p">,</span> <span class="n">z_1</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="s">"""
    Parameters
    ----------
    z_0: lower end of the current binary block
    z_1: higher end of the current binary block
    a: lower end of the current sub-interval
    b: higher end of the current sub-interval
    c: CDF starts with a 0.0
    d: CDF that ends with 1.0

    Returns
    -------
    if [z_0, z_1] is not contained in any of the symbols inside [a, b]:
        return None
    else:
        return the decoded index

    """</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)):</span>
        <span class="n">low</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">low</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">high</span>
        <span class="k">if</span> <span class="n">low</span> <span class="o">&lt;=</span> <span class="n">z_0</span> <span class="ow">and</span> <span class="n">z_1</span> <span class="o">&lt;=</span> <span class="n">high</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">index</span>

</code></pre></div></div>

<h2 id="arithmetic-encoder-with-rescaling-infinite-precision">Arithmetic Encoder with Rescaling (infinite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">a</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">symbol</span><span class="p">],</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span>
    <span class="k">while</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># case 0
</span>            <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
            <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">a</span> <span class="o">*=</span> <span class="mi">2</span>
            <span class="n">b</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># case 1
</span>            <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
            <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># a &lt; 1/2 and b &gt; 1/2
</span>    <span class="k">while</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># a &lt;= 1/4 or b &gt;= 3/4
</span><span class="k">if</span> <span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">:</span>  <span class="c1"># case 2a
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
<span class="k">else</span><span class="p">:</span>  <span class="c1"># case 2b
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
</code></pre></div></div>

<h2 id="arithmetic-encoder-with-rescaling-finite-precision">Arithmetic Encoder with Rescaling (finite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">range_precision</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">a</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">pmf_precision</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">pmf_precision</span>
    <span class="k">while</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="n">range_half</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="n">range_half</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="n">range_half</span><span class="p">:</span>  <span class="c1"># case 0
</span>            <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
            <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">a</span> <span class="o">*=</span> <span class="mi">2</span>
            <span class="n">b</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># case 1
</span>            <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
            <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
    <span class="c1"># a &lt; 1/2 and b &gt; 1/2
</span>    <span class="k">while</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">range_quarter</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">range_quarter</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
<span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># a &lt;= 1/4 or b &gt;= 3/4
</span><span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">range_quarter</span><span class="p">:</span>  <span class="c1"># case 2a
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
<span class="k">else</span><span class="p">:</span>  <span class="c1"># case 2b
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
</code></pre></div></div>

<h2 id="arithmetic-decoder-with-rescaling-finite-precision">Arithmetic Decoder with Rescaling (finite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">range_precision</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">z</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">+=</span> <span class="n">bits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">next_bit_index</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">),</span> <span class="n">range_precision</span><span class="p">)</span>
<span class="n">z_gap</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">range_precision</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">))</span>

<span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">range_precision</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)):</span>
        <span class="n">low</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">low</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">pmf_precision</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">high</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">pmf_precision</span>
        <span class="k">if</span> <span class="n">low</span> <span class="o">&lt;=</span> <span class="n">z</span> <span class="ow">and</span> <span class="n">high</span> <span class="o">&gt;=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">z_gap</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">low</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">high</span>
            <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="k">while</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="n">range_half</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="n">range_half</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="n">range_half</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">b</span>
            <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">a</span>
            <span class="n">z</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">z</span>
            <span class="k">if</span> <span class="n">next_bit_index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
                <span class="n">z</span> <span class="o">+=</span> <span class="n">bits</span><span class="p">[</span><span class="n">next_bit_index</span><span class="p">]</span>
                <span class="n">next_bit_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">z_gap</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
            <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
            <span class="n">z</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">next_bit_index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
                <span class="n">z</span> <span class="o">+=</span> <span class="n">bits</span><span class="p">[</span><span class="n">next_bit_index</span><span class="p">]</span>
                <span class="n">next_bit_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">z_gap</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">range_quarter</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">range_quarter</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">next_bit_index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">+=</span> <span class="n">bits</span><span class="p">[</span><span class="n">next_bit_index</span><span class="p">]</span>
            <span class="n">next_bit_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">z_gap</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[python implementation of arithmetic coding]]></summary></entry><entry><title type="html">Optical Flow – An Overview</title><link href="https://yyang768osu.github.io/blog/2020/optical-flow-an-overview/" rel="alternate" type="text/html" title="Optical Flow – An Overview" /><published>2020-06-16T00:00:00+00:00</published><updated>2020-06-16T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2020/optical-flow-an-overview</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2020/optical-flow-an-overview/"><![CDATA[<ul id="markdown-toc">
  <li><a href="#definition-of-optical-flow" id="markdown-toc-definition-of-optical-flow">Definition of Optical Flow</a></li>
  <li><a href="#useful-resources" id="markdown-toc-useful-resources">Useful Resources</a></li>
  <li><a href="#traditional-approach" id="markdown-toc-traditional-approach">Traditional Approach</a>    <ul>
      <li><a href="#brightness-constancy-assumption" id="markdown-toc-brightness-constancy-assumption">Brightness Constancy Assumption</a></li>
      <li><a href="#small-motion-assumption" id="markdown-toc-small-motion-assumption">Small Motion Assumption</a></li>
      <li><a href="#brightness-constancy-equation" id="markdown-toc-brightness-constancy-equation">Brightness Constancy Equation</a></li>
      <li><a href="#how-to-solve-brightness-constancy-equation" id="markdown-toc-how-to-solve-brightness-constancy-equation">How to solve Brightness Constancy Equation?</a></li>
      <li><a href="#formulation-of-horn-shunck-optical-flow" id="markdown-toc-formulation-of-horn-shunck-optical-flow">Formulation of Horn-Shunck Optical flow</a></li>
      <li><a href="#discrete-optical-flow-estimation" id="markdown-toc-discrete-optical-flow-estimation">Discrete Optical Flow Estimation</a></li>
    </ul>
  </li>
  <li><a href="#dataset" id="markdown-toc-dataset">Dataset</a>    <ul>
      <li><a href="#middlebury-link-paper" id="markdown-toc-middlebury-link-paper">Middlebury (link, paper)</a></li>
      <li><a href="#mpi-sintel-link-paper" id="markdown-toc-mpi-sintel-link-paper">MPI Sintel (link, paper)</a></li>
      <li><a href="#kitti-link-paper" id="markdown-toc-kitti-link-paper">KITTI (link, paper)</a></li>
      <li><a href="#flying-chairs-link-paper" id="markdown-toc-flying-chairs-link-paper">Flying Chairs (link, paper)</a></li>
      <li><a href="#flying-things-3d-link-paper" id="markdown-toc-flying-things-3d-link-paper">Flying Things 3D (link, paper)</a></li>
    </ul>
  </li>
  <li><a href="#evaluation-metric" id="markdown-toc-evaluation-metric">Evaluation Metric</a>    <ul>
      <li><a href="#angular-error-ae" id="markdown-toc-angular-error-ae">Angular Error (AE)</a></li>
      <li><a href="#end-point-error-epe" id="markdown-toc-end-point-error-epe">End Point Error (EPE)</a></li>
    </ul>
  </li>
  <li><a href="#end-to-end-regression-based-optical-flow-estimation" id="markdown-toc-end-to-end-regression-based-optical-flow-estimation">End-to-end regression based optical flow estimation</a>    <ul>
      <li><a href="#some-useful-concepts" id="markdown-toc-some-useful-concepts">Some useful concepts</a></li>
      <li><a href="#overview-of-different-models" id="markdown-toc-overview-of-different-models">Overview of different models</a></li>
      <li><a href="#flownet-iccv-2015-paper" id="markdown-toc-flownet-iccv-2015-paper">FlowNet (ICCV 2015) paper</a></li>
      <li><a href="#flownet-20-cvpr-2017-paper" id="markdown-toc-flownet-20-cvpr-2017-paper">FlowNet 2.0 (CVPR 2017) paper</a></li>
      <li><a href="#spynet-cvpr-2017-paper-code" id="markdown-toc-spynet-cvpr-2017-paper-code">SPyNet (CVPR 2017) paper code</a></li>
      <li><a href="#pwcnet-cvpr-2018-paper-code-video" id="markdown-toc-pwcnet-cvpr-2018-paper-code-video">PWCNet (CVPR 2018) paper code video</a></li>
      <li><a href="#irr-pwcnet-cvpr-2019-paper" id="markdown-toc-irr-pwcnet-cvpr-2019-paper">IRR-PWCNet (CVPR 2019) paper</a></li>
      <li><a href="#pwcnet-fusion-wacv-2019-paper" id="markdown-toc-pwcnet-fusion-wacv-2019-paper">PWCNet Fusion (WACV 2019) paper</a></li>
      <li><a href="#scopeflow-cvpr-2020-paper-code" id="markdown-toc-scopeflow-cvpr-2020-paper-code">ScopeFlow (CVPR 2020) paper code</a></li>
      <li><a href="#maskflownet-cvpr-2020-paper-code" id="markdown-toc-maskflownet-cvpr-2020-paper-code">MaskFlownet (CVPR 2020) paper code</a></li>
      <li><a href="#raft-recurrent-all-pairs-field-transforms-for-optical-flow-arxiv-2020-paper-code" id="markdown-toc-raft-recurrent-all-pairs-field-transforms-for-optical-flow-arxiv-2020-paper-code">RAFT: REcurrent All-Pairs Field Transforms for Optical Flow (Arxiv 2020) paper code</a></li>
    </ul>
  </li>
</ul>

<h2 id="definition-of-optical-flow">Definition of Optical Flow</h2>

<p>Distribution of apparent velocities of movement of brightness pattern in an image.</p>

<ul>
  <li>Where do we need it?
    <ul>
      <li>Action recognition</li>
      <li>Motion segmentation</li>
      <li>Video compression</li>
    </ul>
  </li>
</ul>

<h2 id="useful-resources">Useful Resources</h2>

<ul>
  <li>CMU Computer Vision 16-385
    <ul>
      <li><a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.1_Brightness_Constancy.pdf">Brightness Constancy</a></li>
      <li><a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.2_OF__ConstantFlow.pdf">Optical Flow : Constant Flow</a></li>
      <li><a href="http://www.cs.cmu.edu/~16385/s15/lectures/Lecture21.pdf">Optical Flow : Lucas-Kanade</a></li>
      <li><a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.3_OF__HornSchunck.pdf">Optical Flow : Horn-Shunck</a></li>
    </ul>
  </li>
  <li>CMU Computer Vision 16-720
    <ul>
      <li><a href="http://16720.courses.cs.cmu.edu/lec/motion_lec12.pdf">Motion and Flow</a></li>
      <li><a href="http://16720.courses.cs.cmu.edu/lec/flow.pdf">Estimating Optical Flow 1</a></li>
      <li><a href="http://16720.courses.cs.cmu.edu/lec/flow_lec13.pdf">Estimating Optical Flow 2</a></li>
    </ul>
  </li>
  <li>Papers
    <ul>
      <li><a href="https://arxiv.org/abs/1504.06852">FlowNet: Learning Optical Flow with Convolutional Networks (ICCV 2015)</a></li>
      <li><a href="https://arxiv.org/abs/1612.01925">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks (CVPR 2017)</a></li>
      <li><a href="https://arxiv.org/abs/1611.00850">Optical Flow Estimation using a Spatial Pyramid Network (CVPR 2017)</a></li>
      <li><a href="https://arxiv.org/abs/2004.02853">Optical Flow Estimation in the Deep Learning Age (2020/04/06)</a></li>
      <li><a href="https://arxiv.org/abs/1709.02371">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume (CVPR 2018)</a></li>
      <li><a href="https://arxiv.org/abs/1904.05290">Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation (CVPR 2019)</a></li>
      <li><a href="https://arxiv.org/abs/1810.10066">A fusion approach for multi-frame optical flow estimation (WACV 2019)</a></li>
      <li><a href="https://arxiv.org/abs/2002.10770">ScopeFlow: Dynamic Scene Scoping for Optical Flow (CVPR 2020)</a></li>
      <li><a href="https://arxiv.org/abs/2003.10955">MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask (CVPR 2020)</a></li>
      <li><a href="https://arxiv.org/abs/2003.12039">RAFT: REcurrent All-Pairs Field Transforms for Optical Flow (Arxiv 2020)</a></li>
    </ul>
  </li>
</ul>

<h2 id="traditional-approach">Traditional Approach</h2>

<h3 id="brightness-constancy-assumption">Brightness Constancy Assumption</h3>

\[\begin{align*}
I(x(t), y(t), t) = C
\end{align*}\]

<h3 id="small-motion-assumption">Small Motion Assumption</h3>

\[\begin{align*}
&amp;I(x(t+\delta t), y(t+\delta t), t+ \delta t) = I(x(t), y(t), t) + \frac{dI}{dt}\delta t, \text{(higher order term ignored)}\\
&amp;\text{where }\frac{dI}{dt} = \frac{\partial I}{\partial x}\frac{d x}{d t} + \frac{\partial I}{\partial y}\frac{d y}{d t} + \frac{\partial I}{\partial t}\triangleq I_x u + I_y v + I_t \triangleq \nabla^T I [u, v]^T + I_t
\end{align*}\]

<p>\(\nabla I = [I_x, I_y]^T\) : spatial derivative</p>

<p>\(I_t\) : temporal derivative</p>

<p>\([u, v]\) : optical flow velocities</p>

<h3 id="brightness-constancy-equation">Brightness Constancy Equation</h3>
<p>Combining the above two assumptions, we obtain</p>

\[\begin{align*}
\nabla I [u, v]^T + I_t = 0.
\end{align*}\]

<h3 id="how-to-solve-brightness-constancy-equation">How to solve Brightness Constancy Equation?</h3>
<p>Temporal derivative \(I_t\) can be estimated by frame difference; spatial derivative \(\nabla I\) can be estimated using spatial filters. Since there are two unknowns (\(u\) and \(v\)), the system is under-determined.</p>

<p>Two ways to enforce additional constraints:</p>

<ul>
  <li>Lucas-Kanade Optical Flow (1981) : assuming local patch has constant flow
    <ul>
      <li>LS can be applied to solve this overdetermined set of equations</li>
      <li>If there is lack of spatial gradient in a local path, then the set of equations could still be under-determined. This is referred to as the <code class="language-plaintext highlighter-rouge">aperture</code> problem</li>
      <li>If applied to only tractable patches, these are called sparse flow</li>
    </ul>
  </li>
  <li>Horn-Schunck Optical Flow (1981) : assuming a smooth flow field</li>
</ul>

<h3 id="formulation-of-horn-shunck-optical-flow">Formulation of Horn-Shunck Optical flow</h3>

<p>Brightness constancy constraint/loss :</p>

\[\begin{align*}
E_d(i, j) = \left[I_x(i,j) u(i,j)+I_y(i,j)v(i,j) + I_t(i,j)\right]^2
\end{align*}\]

<p>Smoothness constraint/loss :</p>

\[\begin{align*}
E_s(i, j) = \frac{1}{4}\left[(u(i,j)-u(i+1,j))^2, (u(i,j)-u(i,j+1))^2, (v(i,j)-v(i+1,j)^2, (v(i,j)-v(i,j+1))^2\right]
\end{align*}\]

<p>Solving for optical flow :</p>

\[\begin{align*}
\text{min}_{\bf{u}, \bf{v}} \sum_{i,j} E_d(i,j) + \lambda E_s(i,j)
\end{align*}\]

<p>Gradient descent can be used to solve the above optimization problem.</p>

<h3 id="discrete-optical-flow-estimation">Discrete Optical Flow Estimation</h3>

<p>Brightness Constancy Equation assumes small motion, which is in general not the case. If the movement is beyond 1 pixel, then higher order terms in the Taylor expansion of \(I(x(t), y(t), t)\) could dominate. There are two solutions</p>
<ol>
  <li>To reduce the resolution using coarse-to-fine architecture</li>
  <li>Resort to discrete optical flow estimation</li>
</ol>

<p>For case-2, we obtain optical flow estimate by minimizing the following objective</p>

\[\begin{align*}
&amp;E({\bf{z}}) = \sum_{i\in\mathcal{I}}D(z_i) + \sum_{(i,j)\in \mathcal{N}}S(z_i, z_j)\\
&amp;\text{where } z_i\triangleq (u_i, v_j), \mathcal{I}\triangleq\text{set of all pixels }, \mathcal{N}\triangleq\text{set of all neighboring pixels}
\end{align*}\]

<p>The above can be viewed as energy minimization in a Markov random field.</p>

<h2 id="dataset">Dataset</h2>

<p>Table 1 from <a href="https://arxiv.org/pdf/1504.06852.pdf">FlowNet</a></p>

<table>
  <thead>
    <tr>
      <th>Entry</th>
      <th>Frame Pairs</th>
      <th>Frames with ground truth</th>
      <th>Ground-truth density per frame</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Middlebury</td>
      <td>72</td>
      <td>72</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>KITTI2012</td>
      <td>194</td>
      <td>194</td>
      <td>50%</td>
    </tr>
    <tr>
      <td>MPI Sintel</td>
      <td>1041</td>
      <td>1041</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>Flying Chairs</td>
      <td>22872</td>
      <td>22972</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>Flying Things 3D</td>
      <td>22872</td>
      <td>-</td>
      <td>100%</td>
    </tr>
  </tbody>
</table>

<h3 id="middlebury-link-paper">Middlebury (<a href="http://vision.middlebury.edu/flow/">link</a>, <a href="http://vision.middlebury.edu/flow/floweval-ijcv2011.pdf">paper</a>)</h3>
<p>Contains only 8 image pairs for training, with ground truth flows generated using four different techniques. Displacements are very small, typically below 10 pixels. (Section 4.1 in <a href="https://arxiv.org/pdf/1504.06852.pdf">FlowNet</a>)</p>

<h3 id="mpi-sintel-link-paper">MPI Sintel (<a href="http://sintel.is.tue.mpg.de">link</a>, <a href="http://files.is.tue.mpg.de/black/papers/ButlerECCV2012-corrected.pdf">paper</a>)</h3>

<p>Computer-animated action movie. There are three render passes with varying degree of realism</p>
<ul>
  <li>Albedo render pass</li>
  <li>Clean pass (adds natural shading, cast shadows, specular reflections, and more complex lighting effects)</li>
  <li>Final pass (adds motion blur,  focus blur, and atmospherical effect)</li>
</ul>

<p>Contains 1064 training / 564 withheld test flow fields</p>

<h3 id="kitti-link-paper">KITTI (<a href="http://www.cvlibs.net/datasets/kitti/">link</a>, <a href="http://ww.cvlibs.net/publications/Geiger2013IJRR.pdf">paper</a>)</h3>

<p>Contains 194 training image pairs and includes large displacements, but contains only a very special motion type. The ground truth is obtained from real world scenes by simultaneously recording the scenes with a camera and a 3D laser scanner. This assumes that the scene is rigid and that the motion stems from a moving observer. Moreover, motion of distant objects, such as the sky, cannot be captured, resulting in sparse optical flow ground truth.</p>

<h3 id="flying-chairs-link-paper">Flying Chairs (<a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html">link</a>, <a href="https://arxiv.org/abs/1504.06852">paper</a>)</h3>

<p>Contains about 22k image pairs of chairs superimposed on random background images from Flickr. Random affine transformations are applied to chairs and background to obtain the second image and ground truth flow fields. The dataset contains only planar motions. (Section 3 in <a href="https://arxiv.org/abs/1612.01925">FlowNet 2.0</a>)</p>

<h3 id="flying-things-3d-link-paper">Flying Things 3D (<a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html">link</a>, <a href="https://arxiv.org/pdf/1512.02134.pdf">paper</a>)</h3>

<p>A natural extension of the FlyingChairs dataset, having 22,872 larger 3D scenes with more complex motion patterns.</p>

<h2 id="evaluation-metric">Evaluation Metric</h2>

<h3 id="angular-error-ae">Angular Error (AE)</h3>
<p>AE between \((u_0, v_0)\) and \((u_1, v_1)\) is the angle in 3D space between \((u_0, v_0, 1.0)\) and \((u_1, v_1, 1.0)\). Error in large flow is penalized less than errors in small flow. (Section 4.1 in <a href="http://vision.middlebury.edu/flow/flowEval-iccv07.pdf">link</a>)</p>

<h3 id="end-point-error-epe">End Point Error (EPE)</h3>
<p>EPE between \((u_0, v_0)\) and \((u_1, v_1)\) is \(\sqrt{(u_0-u_1)^2 + (v_0-v_1)^2}\) (Euclidean distance).</p>

<p>For Sintel MPI, papers also often reports detailed breakdown of EPE for pixels with different distance to motion boundaries (\(d_{0-10}\), \(d_{10-60}\), \(d_{60-140}\)) and different velocities (\(s_{0-10}\), \(s_{10-40}\), \(s_{40+}\)).</p>

<h2 id="end-to-end-regression-based-optical-flow-estimation">End-to-end regression based optical flow estimation</h2>

<h3 id="some-useful-concepts">Some useful concepts</h3>

<ul>
  <li>Backward warping</li>
</ul>

<p>\(I_1(\cdot, \cdot)\triangleq I(\cdot, \cdot, t=1), I_2(\cdot, \cdot)\triangleq I(\cdot, \cdot, t=2)\). Optical flow field \(u, v\) satisfies \(I_1(x, y) = I_2(x+u, y+v)\). In other words, \(u,v\) tells us where each pixel in \(I_1\) is coming from, compared with \(I_2\), and given \(u, v\), we know how to move around (warp) the pixels in \(I_2\) to obtain \(I_1\). Here \(I_1\) is often referred to as the source image and \(I_2\) the target image – flow vector is defined per source image. Specifically, we can define a <code class="language-plaintext highlighter-rouge">warp</code> operation as below</p>

\[\begin{align*}
&amp;I_{\text{source}} = \texttt{warp}(I_\text{target}, f) \text{ where}\\
&amp;I_{\text{source}}(x, y) = I_\text{target}(x+u, y+v)
\end{align*}\]

<ul>
  <li>Compositivity of backward warping</li>
</ul>

\[\begin{align*}
\texttt{warp}(I_\text{target}, f_a+f_b) = \texttt{warp}\left(\texttt{warp}(I_\text{target}, f_a), f_b\right)
\end{align*}\]

<h3 id="overview-of-different-models">Overview of different models</h3>

<table>
  <thead>
    <tr>
      <th>Model Name</th>
      <th>Num of parameters</th>
      <th>inference speed</th>
      <th>Training time</th>
      <th>MPI Sintel final test EPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>FlowNetS</td>
      <td>32M</td>
      <td>87.72fps</td>
      <td>4days</td>
      <td>7.218</td>
    </tr>
    <tr>
      <td>FlowNetC</td>
      <td>32M</td>
      <td>46.10fps</td>
      <td>6days</td>
      <td>7.883</td>
    </tr>
    <tr>
      <td>FlowNet2.0</td>
      <td>162M</td>
      <td>11.79fps</td>
      <td>14days</td>
      <td>6.016</td>
    </tr>
    <tr>
      <td>SPyNet</td>
      <td>1.2M</td>
      <td>-</td>
      <td>-</td>
      <td>8.360</td>
    </tr>
    <tr>
      <td>PWCNet</td>
      <td>8.7M</td>
      <td>35.01fps</td>
      <td>4.8days</td>
      <td>5.042</td>
    </tr>
  </tbody>
</table>

<p class="notice">The EPE column is taken from Table 2 of <a href="https://arxiv.org/abs/2004.02853">an overview paper</a>. The inference speed (on Pascal Titan X) and training time column is taken from Table 7 of <a href="https://arxiv.org/abs/1709.02371">PWCNet paper</a>.</p>

<h3 id="flownet-iccv-2015-paper">FlowNet (ICCV 2015) <a href="https://arxiv.org/abs/1504.06852">paper</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_encoder-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_encoder-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_encoder-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/FlowNet_encoder.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_decoder-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_decoder-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_decoder-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/FlowNet_decoder.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>The first end-to-end CNN architecture for estimating optical flow. Two variants:</p>
<ul>
  <li>FlowNetS
    <ul>
      <li>A pair of input images is simply concatenated and then input to the U-shaped network that directly outputs optical flow.</li>
    </ul>
  </li>
  <li>FlowNetC
    <ul>
      <li>FlowNetC has a shared encoder for both images, which extracts a feature map for each input image, and a cost volume is constructed by measuring patch-level similarity between the two feature maps with a correlation operation. The result is fed into the subsequent network layers.</li>
    </ul>
  </li>
</ul>

<p>Multi-scale training loss is applied. Both models still under-perform energy-based approaches.</p>

<h3 id="flownet-20-cvpr-2017-paper">FlowNet 2.0 (CVPR 2017) <a href="https://arxiv.org/abs/1612.01925">paper</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_2-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_2-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_2-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/FlowNet_2.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key ideas:</p>
<ol>
  <li>By stacking multiple FlowNet style networks, one can sequentially refine the output from previous network modules.</li>
  <li>It is helpful to pre-train networks on a less challenging synthetic dataset first and then further train on a more challenging synthetic dataset with 3D motion and photometric effects</li>
</ol>

<p>End-to-end based approach starts to outperform energy-based ones.</p>

<h3 id="spynet-cvpr-2017-paper-code">SPyNet (CVPR 2017) <a href="https://arxiv.org/abs/1611.00850">paper</a> <a href="https://github.com/anuragranj/spynet">code</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/SPyNet-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/SPyNet-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/SPyNet-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/SPyNet.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key idea:</p>
<ul>
  <li>Incorporate classic <code class="language-plaintext highlighter-rouge">coarse-to-fine</code> concepts into CNN network and update residual flow over mulitple pyramid levels (5 image pyramid levels are used). Networks at different levels have separate parameters.</li>
</ul>

<p>Achieves comparable performance to FlowNet with 96% less number of parameters.</p>

<h3 id="pwcnet-cvpr-2018-paper-code-video">PWCNet (CVPR 2018) <a href="https://arxiv.org/abs/1709.02371">paper</a> <a href="https://github.com/NVlabs/PWC-Net">code</a> <a href="https://www.youtube.com/watch?v=vVU8XV0Ac_0">video</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/PWCNet-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/PWCNet-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/PWCNet-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/PWCNet.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key ideas:</p>
<ol>
  <li>Learned feature pyramid instead of image pyramid</li>
  <li>Warping of feature maps</li>
  <li>Computing a cost volume of learned feature maps (correlation)</li>
</ol>

<p>Computation steps:</p>
<ol>
  <li>Feature pyramid extractor: conv-net with down-sampling</li>
  <li>Target feature map is warped by up-sampled previous flow estimation</li>
  <li>Cost volume is computed based on source feature map and warped target feature map</li>
  <li>Optical flow estimator: a DenseNet type of network that takes (1) source feature map (2) cost volume (3) up-sampled previous optical flow estimate</li>
  <li>Context network: a dilated convolution network to post process the estimated optical flow</li>
</ol>

<p>Remarks:</p>
<ul>
  <li>Multi-scale training loss</li>
  <li>Network at each scale estimates the optical flow for that scale, not the residual optical flow (the addition happens implicitly inside the optical flow estimator).</li>
</ul>

<h3 id="irr-pwcnet-cvpr-2019-paper">IRR-PWCNet (CVPR 2019) <a href="https://arxiv.org/abs/1904.05290">paper</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/IRR_PWCNet-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/IRR_PWCNet-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/IRR_PWCNet-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/IRR_PWCNet.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key ideas</p>
<ul>
  <li>Take the output from a previous pass through the network as input and iteratively refine it by only using a single network block with shared weights, which allows the network to residually refine the previous estimate.</li>
  <li>For PWCNet, the decoder module at different pyramid level is achieved using a 1x1 convolution before feeding the source feature map to the optical flow estimator/decoder.</li>
  <li>Joint occlusion and bidirectional optical flow estimation leads to further performance enhancement.</li>
</ul>

<h3 id="pwcnet-fusion-wacv-2019-paper">PWCNet Fusion (WACV 2019) <a href="https://arxiv.org/abs/1810.10066">paper</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/PWCNet_Fusion-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/PWCNet_Fusion-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/PWCNet_Fusion-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/PWCNet_Fusion.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>The paper focuses on three-frame optical flow estimation problem: given \(I_{t-1}\), \(I_{t}\), and \(I_{t+1}\), estimate \(f_{t\to t+1}\).</p>

<p>Key ideas:</p>
<ul>
  <li>If we are given \(f_{t-1\to t}\) and \(f_{t\to t-1}\), and assume constant velocity of movement, then an estimate of \(f_{t\to t+1}\) can be formed by backward warping \(f_{t-1\to t}\) with \(f_{t\to t-1}\).
\(\begin{align*}
&amp;\widehat{f}_{t\to t+1} \triangleq \texttt{warp}(f_{t-1 \to t}, f_{t\to t-1}), \\
&amp;\widehat{f}_{t\to t+1}(x, y) \triangleq f_{t-1 \to t}\left(x+f_{t\to t-1}(x,y)(x), y+f_{t\to t-1}(x,y)(y)\right)
\end{align*}\)</li>
  <li>With three frames available, we can plug-in any two-frame optical flow estimation solution (PWCNet in this case) to obtain \(f_{t-1 \to t}\), \(f_{t\to t+1}\) and \(f_{t \to t-1}\).</li>
  <li>A fusion network (similar to the one used in the last stage of FlowNet 2.0) can be used to fuse together \(\widehat{f}_{t \to t-1}\triangleq\texttt{warp}(f_{t-1 \to t}, f_{t\to t-1})\) and \(f_{t \to t+1}\).
    <ul>
      <li>Note that \(\widehat{f}_{t\to t-1}\) would be identical to \(f_{t\to t+1}\) if (a) velocity is constant (b) three optical flow estimations are correct, and (c) there are no occlusions. Brightness constancy errors of the two flow maps together with the source frame \(I_t\) are fed into the fusion network to provide additional info.</li>
    </ul>
  </li>
</ul>

<p>Why multi-frame may perform better than 2-frame solutions:</p>
<ul>
  <li>temporal smoothness leads to additional regularization.</li>
  <li>longer time sequences may help in ambiguous situations such as occluded regions.</li>
</ul>

<h3 id="scopeflow-cvpr-2020-paper-code">ScopeFlow (CVPR 2020) <a href="https://arxiv.org/abs/2002.10770">paper</a> <a href="https://github.com/avirambh/ScopeFlow">code</a></h3>

<p>ScopeFlow revisits the following two parts in the conventional end-to-end training pipeline/protocol</p>
<ul>
  <li>Data augmentation:
    <ol>
      <li>photometric transformations: input image perturbation, such as color and gamma corrections.</li>
      <li>geometric augmentations: global or relative affine transformation, followed by random horizontal and vertical flipping.</li>
      <li>cropping</li>
    </ol>
  </li>
  <li>Regularization
    <ul>
      <li>weighted decay</li>
      <li>adding random Gaussian noises</li>
    </ul>
  </li>
</ul>

<p>and advocates</p>
<ul>
  <li>use larger scopes (crops and zoom-out) when possible.</li>
  <li>gradually reduce regularization</li>
</ul>

<h3 id="maskflownet-cvpr-2020-paper-code">MaskFlownet (CVPR 2020) <a href="https://arxiv.org/abs/2003.10955">paper</a> <a href="https://github.com/microsoft/MaskFlownet">code</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/AsymOFMM-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/AsymOFMM-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/AsymOFMM-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/AsymOFMM.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/MaskFlowNetS-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/MaskFlowNetS-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/MaskFlowNetS-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/MaskFlowNetS.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key idea:</p>
<ul>
  <li>Incorporates a learnable occlusion mask that filters occluded areas immediately after feature warping without any explicit supervision.</li>
</ul>

<h3 id="raft-recurrent-all-pairs-field-transforms-for-optical-flow-arxiv-2020-paper-code">RAFT: REcurrent All-Pairs Field Transforms for Optical Flow (Arxiv 2020) <a href="https://arxiv.org/abs/2003.12039">paper</a> <a href="https://github.com/princeton-vl/RAFT">code</a></h3>]]></content><author><name></name></author><summary type="html"><![CDATA[summary of how optimal flow can be derived]]></summary></entry><entry><title type="html">Markov Chain Monte Carlo: Gibbs, Metropolis-Hasting, and Hamiltonian</title><link href="https://yyang768osu.github.io/blog/2019/markov-chain-monte-carlo/" rel="alternate" type="text/html" title="Markov Chain Monte Carlo: Gibbs, Metropolis-Hasting, and Hamiltonian" /><published>2019-07-06T00:00:00+00:00</published><updated>2019-07-06T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2019/markov-chain-monte-carlo</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2019/markov-chain-monte-carlo/"><![CDATA[<p>A fundamental problem in statistical learning is to compute the expectation with respect to some target probability distribution \(\pi\)</p>

\[\begin{align*}
\mathbb{E}_\pi\left[f\right] \triangleq \int \pi(x) f(x) dx.
\end{align*}\]

<p>There are two difficulties in the evaluation of the above (1) often \(\pi(\cdot)\) is available to us only as a form of unnormalized probability, i.e., it can be evaluated only up to a normalizing constant (2) even if \(\pi(\cdot)\) can be evaluated exactly, it is often hard to directly generate samples from it (e.g., for high-dimensional space).</p>

<p>One example application is Bayesian inference, where the posterior probability of the latent \(\pi(x\|D)\) is available only in the form of prior \(\pi(x)\) times likelihood \(\pi(D\|x)\) up to the unknown normalizing constant of \(\pi(D)\), and we would like to either sample or obtain the expectation with respect to the posterior probability.</p>

<p>The idea of Markov Chain Monte Carlo (MCMC) is to construct a Markov chain whose stationary distribution is exactly the target distribution with easy-to-sample transition kernels. One could then start with a random initial state, and yield samples by simply running the transitions and use the generated samples after the chain reaches steady state for the Monte Carlo evaluation of the expectation.</p>

<p>For the design of such Markov chain, all methods that I encountered utilize the following theorem</p>
<blockquote>
  <p>An irreducible and aperiodic Markov chain with transition probability \(P\) has stationary distribution of \(\pi\) if it satisfies \begin{align} 
\pi(x)P(x’|x) = \pi(x’)P(x|x’) \notag
\end{align}</p>
</blockquote>

<p>The game, then, is to design \(P\) for which the above equality holds. In this article, we will go through three MCMC methods with different ways in the design of \(P\), namely <strong>Gibbs sampling</strong>, <strong>Metropolis-Hastings</strong>, and <strong>Hamiltonian Monte Carlo</strong> (HMC).</p>

<p>As a side note, it is worth pointing out that the above equation, referred to as <em>detailed balance equation</em>, is a sufficient but not necessary condition for a Markov chain to have stationary distribution \(\pi\). It defines a special case of Markov chain called reversible Markov chain. The detailed balance equation should be contrasted with <em>global balance equation</em> below, which all Markov chains with stationary distribution \(\pi\) satisfy. Then it shouldn’t be surprising that global balance equation can be easily derived from detailed balance equation (by summing over \(x'\) on both sides of the above equation) but not the other way around.</p>

\[\begin{align*}
\pi(x) = \sum_{x'} \pi(x')P(x'|x).
\end{align*}\]

<h2 id="gibbs-sampling">Gibbs sampling</h2>

<p>In Gibbs sampling, the transition probability \(P\) is defined as the following</p>

\[\begin{align*}
P\left(x'|x\right)=\left\{
\begin{array}{ll}
\frac{1}{d}\pi\left(x'_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d\right) &amp; \text{if there exits }j\text{ such that }x_i'=x_i\text{ for }i\not=j.\\
0&amp;\text{otherwise.}
\end{array}
\right.
\end{align*}\]

<p>The state \(x\) is a vector of dimension \(d\), and the transition probability from state \(x\) to state \(x'\) is non-zero when they differ by only one dimension, say dimension \(j\), and the transition probability is designed to be the conditional probability of \(x'_j\), given all the other dimensions fixed, scaled by \(1/d\). This corresponds to a transition scheme where we uniformly pick a dimension \(j\), and then randomly sample a value in dimension \(j\) following the conditional distribution. Detailed balance equation holds with such design</p>

\[\begin{align*}
&amp;\pi(x)P(x'|x)\\
=&amp;\frac{1}{d}\pi(x)\pi(x'_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d)\\
=&amp; \frac{1}{d}\pi(x)\pi(x')/\sum_{z}\pi(x_1, \ldots, x_{j-1}, z, x_{j+1}, \ldots, x_d)\\
=&amp;\frac{1}{d}\pi(x)\pi(x')/\sum_{z}\pi(x'_1, \ldots, x'_{j-1}, z, x'_{j+1}, \ldots, x'_d)\\
=&amp;\frac{1}{d}\pi(x')\pi(x_j|x'_1, \ldots, x'_{j-1}, x'_{j+1}, \ldots, x'_d)\\
=&amp;\pi(x')P(x|x').
\end{align*}\]

<p>The premise of Gibbs sampling is that the conditional distribution of one dimension given the rest is much easier to normalize and sample from. It is quite limited though, in the sense that the transition can never go very far in each step – only one dimension can be changed at a time. As a consequence, the transition matrix is quite sparse and the Markov chain may suffer from very large mixing time (time to stationary distribution) and it may not scale well with large dimensional space.</p>

<h2 id="metropolis-hastings">Metropolis Hastings</h2>

<p>Metropolis Hastings algorithm is a much more general version of Gibbs; in fact it encompasses both Gibbs sampling and Hamiltonian MC as special realizations. The basic idea is to construct the transition distribution from a flexible form of proposal distribution \(g(x'\|x)\), corrected by a <em>acceptance ratio</em> term \(A(x',x)\)  to guarantee reversibility in time. Specifically, the acceptance ratio is chosen to enforce the detailed balance equation</p>

\[\begin{align*}
\pi(x) g(x'|x) A(x', x) = \pi(x') g(x|x') A(x, x').
\end{align*}\]

<p>The actual transition probability is then \(P(x'\|x) \triangleq g(x'\|x) A(x', x)\), corresponding to a sampling scheme where we first sample from \(g(x'\|x)\) to have a candidate next state \(x'\), and then accept this candidate with probability \(A(x', x)\). If the candidate state is rejected, the next state will remain the same as the current state. For an arbitrary proposal distribution \(g\), from the above equation, we have</p>

\[\begin{align*}
\frac{A(x', x)}{A(x, x')} = \frac{\pi(x')g(x|x')}{\pi(x)g(x'|x)}.
\end{align*}\]

<p>To reduce the mixing time of the Markov chain, it is desirable to maximize the acceptance ratio \(A\). This means that we want to set either \(A(x',x)\) or \(A(x, x')\) to be \(1\) for any pair of \(x\) and \(x'\), resulting in the expression below</p>

\[\begin{align*}
A(x', x) = \min\left\{1, \frac{\pi(x')g(x|x')}{\pi(x)g(x'|x)}\right\}.
\end{align*}\]

<p>In the above equation, since \(\pi\) appear in both numerator and denominator, we can easily work with unnormalized probability distribution, as long as it can be evaluated efficiently for each data point.</p>

<p>Metropolis-Hasting algorithm itself is just a MCMC framework; it still relies on a good choice of proposal distribution to perform well. The design of \(g\) can be problem specific and is the <em>art</em>. The clear optimal choice of is \(g(x'\|x)=\pi(x)\), which degenerates to the direct sampling of \(\pi\) with acceptance ratio of \(1\).</p>

<h2 id="hamiltonian-monte-carlo">Hamiltonian Monte Carlo</h2>

<p>Let’s now image a high dimensional surface for which the potential energy at each point \(x\) is defined as \(V(x)\triangleq -\log\pi(x)\). Here we introduce an auxiliary variable \(p\) with the same dimension as \(x\), and interpret the pair of variable \((x, p)\) as describing the position and momentum of an object on the high dimensional space.</p>

<p>The kinetic energy of the object with mass \(m\) and momentum \(p\) is known as \(K(p)=\frac{p^2}{2m}\) (e.g., \(\frac{1}{2}mv^2 = (mv)^2/2m\)). We now construct a joint probability distribution of \((x,p)\) as</p>

\[\begin{align*}
\pi(x, p) = \frac{1}{Z}e^{-V(x)-K(p)} = \frac{1}{Z} e^{\log\pi(x)}e^{p^2/2m} = \frac{1}{Z}\pi(x)\mathcal{N}\left(p|0, \sqrt{m}\right).
\end{align*}\]

<p>Two remarks here: (1) The joint probability defined above is a function of the total energy \(V(x) + K(p)\) (potential energy plus kinetic energy) of the imaginary object. (2) Since the marginal distribution of \(\pi(x, p)\) with respect to \(x\) is \(\pi(x)\), if we can construct an effective MCMC algorithm for \((x, p)\), we then obtain an MCMC algorithm for \(x\) by discarding \(p\).</p>

<p>The key in Hamiltonian MC is to use Hamiltonian mechanism as a way to obtain new candidate state (corresponding to proposal \(g\) in Metropolis-Hastings).  Hamiltonian mechanics is an alternative reformation of the classic Newtonian mechanics describing Newton’s second law of motion. It characterizes the time evolution of the system in terms of location \(x\) and momentum \(p\), with the conservation of the sum of potential energy \(V(x)\) and Kinetic energy of \(K(p)\), a.k.a. Hamiltonian \(\mathcal{H}(x, p) \triangleq V(x) + K(p)\), through the following differential equations</p>

\[\begin{align*}
\frac{d p}{dt} =&amp; -\frac{\partial \mathcal{H}}{\partial x} &amp;\text{force equals to negative gradient of potential energy}\\
\frac{d x}{dt} =&amp; \frac{\partial \mathcal{H}}{\partial p} &amp;\text{velocity equals to derivative of kinetic energy w.r.t. momentum}
\end{align*}\]

<p>By solving the path of \((x, p)\) according to Hamiltonian mechanics, we are essentially traversing along the contour for which \(\pi (x, p)\) is fixed. This provide a very nice way of coming up with a proposal function \(g(x', p'\| x, p)\) without having to reject any candidate. In other words, if we start with the point \((x, p)\) and derive the system state \((x_\tau, p_\tau)\) after a period of time \(\tau\) , we then know that \(\pi(x, p) = \pi(x_\tau, p_\tau)\). If we further apply a negation in the momentum, then the proposal function is reversible.</p>

\[\begin{align*}
x, p \xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau \xrightarrow[]{\substack{\text{negate}\\\text{momentum}}} x_\tau, -p_\tau  = x', p'\\
x', p' \xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x'_\tau, p'_\tau \xrightarrow[]{\substack{\text{negate}\\\text{momentum}}}x'_\tau, -p'_\tau  = x, p
\end{align*}\]

<p>If we have perfect solver for the differential equation, then according to Equation (2) there is no need to reject any transition proposal. However, in reality the differential equation can only be solved in approximation with error, and thus \(\pi(x, p)\not=\pi(x', p')\), meaning that the acceptance ratio is not strictly \(1\) and certain fraction of the transition proposal would be rejected. It is worth noting that the method for computing the solution to the differential equation should still be reversible to respect the detailed balance equation. One hidden condition for such transition to be feasible is that the potential energy \(V(\cdot)\) has to be differentiable, implying that the target distribution \(\pi(\cdot)\) should be differentiable.</p>

<p>So now we have defined a proposal function according to Hamiltonian mechanics, which leads to large acceptance ratio. Are we done here? Not yet. If we stop here, then the Markov chain we defined is reducible, i.e., not every state is accessible from an initial state. In fact, we only have pairwise transition in the Markov chain. To ensure the sampling of the entire space, another proposal distribution \(g_2\) is introduced, taking advantage of the fact that \(\pi(x, p)\) has factorized form for which \(p\) follows a zero-mean normal distribution – the proposal distribution \(g_2\) simply samples the momentum value \(p\) from the corresponding marginal distribution. For such proposal, the corresponding acceptance ratio is \(1\)</p>

\[\begin{align*}
A((x, p'), (x, p)) = \min\left\{1, \frac{\pi(x, p')g_2(p|p')}{\pi(x, p)g_2(p'|p)}\right\} = \min\left\{1, \frac{\pi(x)}{\pi(x)}\right\}=1 .
\end{align*}\]

<p>Now we concatenate the above two proposals to have the final form of Hamiltonian MC sampling</p>

\[\begin{align*}
x, p_0 \xrightarrow[]{\substack{\text{resample}\\\text{momentum}}} x, p 
\xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau 
\xrightarrow[]{\substack{\text{negate}\\\text{momentum}}} x_\tau, -p_\tau  = x', p'.
\end{align*}\]

<p>Since every time after applying the Hamiltonian mechanics the momentum is resampled, we can ignore the momentum negation operation, leading to the following</p>

\[\begin{align*}
x \xrightarrow[]{\substack{\text{resample}\\\text{momentum}}} x, p 
\xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau 
\xrightarrow[]{\substack{\text{discard}\\\text{momentum}}} x_\tau = x',
\end{align*}\]

<p>and the corresponding acceptance ratio is</p>

\[\begin{align*}
A((x_\tau, p_\tau), (x, p)) = \min\left\{1, \frac{\pi(x_\tau, p_\tau)}{\pi(x, p)}\right\} =  \min\left\{1, e^{\mathcal{H}(x, p) - \mathcal{H}(x_\tau, p_\tau)}\right\}.
\end{align*}\]]]></content><author><name></name></author><summary type="html"><![CDATA[a primer on Gibbs sampling, Metropolis-Hasting, and Hamiltonian MC]]></summary></entry><entry><title type="html">Normalizing Flow: understanding the change of variable equation</title><link href="https://yyang768osu.github.io/blog/2019/normalizing-flow-change-of-variable-equation/" rel="alternate" type="text/html" title="Normalizing Flow: understanding the change of variable equation" /><published>2019-03-15T00:00:00+00:00</published><updated>2019-03-15T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2019/normalizing-flow-change-of-variable-equation</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2019/normalizing-flow-change-of-variable-equation/"><![CDATA[<p>Normalizing flow is a technique for constructing complex probability distributions through invertible transformations of a simple distribution. It has been studied and applied in generative models under two contexts: (1) characterizing the approximation posterior distribution of latent variables in the case of variational inference (2) directly approximating the data distribution. When used in the second context, it has demonstrated its capability in generating high-fidelity audio, image, and video data.</p>

<p>The study of generative models is all about learning a distribution \(\mathbb{P}_{\mathcal{X}}\)  that fits the data \(\mathcal{X}\) well. With such distribution \(\mathbb{P}(\mathcal{X})\) we can, among other things, generate, by sampling from  \(\mathbb{P}_{\mathcal{X}}\), artificial data point that resembles \(\mathcal{X}\). Since the true data distribution lies in high-dimensional space and is potentially very complex, it is essential to have a parameterized distribution family that is flexible and expressive enough to approximate the true data distribution well.</p>

<p>The idea of flow-based methods is to <em>explicitly</em> construct a parameterized family of distributions by transforming a known distribution \(\mathbb{P}_{\mathcal{Z}}\), e.g., a standard multi-variant Gaussian, through a concatenation of function mappings. Let’s consider the elementary case of a single function mapping \(g\). For each sampled value \(z\) from \(\mathbb{P}_{\mathcal{Z}}\), we map it to a new value \(x=g(z)\).</p>

\[\begin{align*}
z \xrightarrow{g(.)}  x
\end{align*}\]

<p>Up until this point, we have not introduced anything new. This way of transforming a known distribution using a function mapping \(g\) is also adopted by generative adversarial networks (GAN). The question that flow-based method asks is: can we get a tractable probability density function (pdf) of \(x=g(z)\)? If so, we can <em>directly</em> optimize the probability density of the dataset, i.e., the log likelihood of the data, rather than resorting to the duality approach adopted by GAN, or the lower-bound approach adopted by VAE.</p>

<p>Unfortunately, for a general function \(g\) that maps \(z\) to \(x\), the pdf of the new random variable \(x=g(z)\) is quite complicated and usually intractable due to the need to calculate a multi-dimensional integral. However, if we restrict \(g\) to be a bijective (one-to-one correspondence) and differentiable function, then the general change-of-variable technique reduces to the following tractable form:</p>

\[\begin{align*}
\mathbb{P}_\mathcal{X}(x) =  \mathbb{P}_{\mathcal{Z}}(z)\left|\det \frac{d g(z)}{d z}\right|^{-1}, x=g(z)
\end{align*}\]

<p>An important consequence with the bijective assumption is that \(z\) and \(x\) must have the same dimension: if \(z\) is a \(d-\)dimensional vector \(z=[z_1, z_2, \ldots, z_d]\), the corresponds \(x\) must also be a \(d-\)dimensional vector \(x=[x_1, x_2, \ldots, x_d]\). It is worth emphasizing that the bijective assumption is essential to the tractability of the change-of-variable operation, and the resulting dimension invariance is a key restriction in flow-based methods.</p>

<p>The above equation, albeit tractable, looks by no means familiar or friendly — what is with the absolute value? the determinant? the Jacobian? the inverse? The whole equation screams for an intuitive explanation. So here we go — let’s gain some insights into the meaning of the formula.</p>

<p>First off, since \(g\) is bijective and thus invertible, we can denote the inverse of \(g\) as \(f=g^{-1}\), which allows us to rewrite the equation as</p>

\[\begin{align*}
\mathbb{P}_\mathcal{X}(x) =  \mathbb{P}_{\mathcal{Z}}(f(x))\left|\det \frac{d x}{d f(x)}\right|^{-1} =  \mathbb{P}_{\mathcal{Z}}(f(x))\left|\det \frac{d f(x)}{d x}\right|
\end{align*}\]

<p>In the last equation, we get ride of the inverse by resorting to the identity that the determinant of an inverse is the inverse of the determinant, the intuition of which will become clear later.</p>

<p>To understand the above equation, we start with a fundamental invariance in the change of probability random variables: <strong>the probability mass of the random variable \(z\) in any subset of \(\mathcal{Z}\) must be the same as the probability mass of \(x\) in the corresponding subset of \(\mathcal{X}\) induced by transformation from \(z\) to \(x\)</strong>, and vice versa.</p>

<p>Let us exemplify the above statement with an example. Consider the case when \(x\) and \(z\) are 2 dimensional, and focus on a small rectangular in \(\mathcal{X}\) defined by two corner points \((a, b)\) and \((a+\Delta x_1, b + \Delta x_2)\). If \(\Delta x_1\) and \(\Delta x_2\) are small enough, we can approximate the probability mass on the rectangular as the density \(\mathbb{P}_\mathcal{X}\) evaluated at point \((a,b)\) times the area of the rectangular. More precisely,</p>

\[\begin{align*}
&amp;P {\big(}  (x_1, x_2) \in [a, a+\Delta x_1]\times[b, b+\Delta x_2] {\big)}\\
\approx&amp; \mathbb{P}_\mathcal{X} ((a, b)) \times \text{area of }[a, a+\Delta x_1]\times[b, b+\Delta x_2]\\
=&amp;\mathbb{P}_\mathcal{X} ((a, b)) \Delta x_1 \Delta x_2
\end{align*}\]

<p>This approximation is basically assuming that the probabilistic density on the rectangular stays constant and equal to \(\mathbb{P}_\mathcal{X} ((a, b))\), which holds asymptotically true as we shrink the width \(\Delta x_1\) and height \(\Delta x_2\) of the rectangular. The left figure below provides an illustration.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/normalizing_flow/flow-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/normalizing_flow/flow-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/normalizing_flow/flow-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/normalizing_flow/flow.png" />

  </picture>

</figure>

    </div>
</div>

<p>Now resorting to the aforementioned invariance, the probability mass on the \(\Delta x_1 \times \Delta x_2\) rectangular must remain unchanged after the transformation. So what does the rectangular look like after the transformation of \(f\)? Let us focus on the corner point \((a+\Delta x_1, b)\):</p>

\[\begin{align*}
f((a+\Delta x_1, b))=&amp;(f_1(a+\Delta x_1,b), f_2(a+\Delta x_1,b))  \\
=&amp; (f_1(a,b), f_2(a,b))  \\
+&amp; \left(\frac{\partial f_1}{\partial x_1}(a, b)\Delta x_1 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_1\right) \text{ }\text{ first order component} \\
+&amp; \left(o(\Delta x_1), o(\Delta x_1)\right) \text{ }\text{ second and higher order residual}
\end{align*}\]

<p>With \(\Delta x_1\) and \(\Delta x_2\) small enough, we can just ignore the higher order term and keep the linearized term. As can be seen from the figure above, the rectangular area is morphed into a parallelogram defined by the two vectors</p>

\[\begin{align*}
&amp;\left(\frac{\partial f_1}{\partial x_1}(a, b)\Delta x_1 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_1\right)\\
&amp;\left(\frac{\partial f_1}{\partial x_2}(a, b)\Delta x_2 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_2\right)
\end{align*}\]

<p>We have <a href="https://textbooks.math.gatech.edu/ila/determinants-volumes.html">geometry</a> to tell us that the area of a parallelogram is just the absolute determinant of the matrix composed of the edge vectors, which is expressed as below.</p>

\[\begin{align*}
{\Bigg|} \det \underbrace{\left[
\begin{array}{ll}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_1}\\
\frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_2}
\end{array}
\right]_{(a,b)}}_{\substack{\text{Jacobian of $f$}\\\text{evaluated at $(a,b)$}}}
{\Bigg |} 
\Delta x_1 \Delta x_2
\end{align*}\]

<p>By plugging in the above into the invariance statement, we reached the following identity</p>

\[\begin{align*}
\mathbb{P}_\mathcal{X} ((a, b)) \Delta x_1 \Delta x_2 = \mathbb{P}_\mathcal{Z} (f(a, b)) \left|\det {\bf J}_f(a,b)\right| \Delta x_1 \Delta x_2
\end{align*}\]

<p>With \(\Delta x_1\Delta x_2\) canceled out, we reached our target equation</p>

\[\begin{align*}
\mathbb{P}_\mathcal{X} (x) = \mathbb{P}_\mathcal{Z} (f(x)) \left|\det {\bf J}_f(x)\right| = \mathbb{P}_\mathcal{Z} (f(x)) \left|\det \frac{\partial f(x)}{\partial x}\right|.
\end{align*}\]

<p>For data with dimension larger than two, the above equation still holds, with the distinctions that the parallelogram becomes a parallelepiped, and the concept of area becomes a more general notion of volume.</p>

<p>It should be clear now what the physical interpretation is for the absolute-determinant-of-Jacobian — it represents the <strong>local, linearized rate of volume change</strong> (quoted from <a href="https://blog.evjang.com/2018/01/nf1.html">this excellent blog</a>) for the function transform. Why do we care about the rate of volume change? exactly because of the invariance of probability measure — in order to make sure each volume holds the same measure of probability before and after the transformation, we need to factor in the volume change induced by the transformation.</p>

<p>With this interpretation that the absolute-determinant-of-Jacobian is just local linearized rate of volume change, it should not be surprising that the determinant of a Jacobian of an inverse function is the inverse of the determinant Jacobian of the original function. In other words, if function \(f\) expands a volume around \(x\) by rate of \(r\), then the inverse function \(g=f^{-1}\) must shrink a volume around \(f(x)\) by the same rate of \(r\).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[decipher absolute-logarithm-determinant-Jocabian]]></summary></entry><entry><title type="html">Understanding conventional HMM-based ASR training</title><link href="https://yyang768osu.github.io/blog/2018/understanding-conventional-hmm-based-asr-training/" rel="alternate" type="text/html" title="Understanding conventional HMM-based ASR training" /><published>2018-11-17T00:00:00+00:00</published><updated>2018-11-17T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2018/understanding-conventional-hmm-based-asr-training</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2018/understanding-conventional-hmm-based-asr-training/"><![CDATA[<p>Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here \(\theta\) denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/asr/HMM-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/asr/HMM-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/asr/HMM-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/asr/HMM.png" />

  </picture>

</figure>

    </div>
</div>

<h2 id="maximum-likelihood-training">Maximum likelihood training</h2>

<p>In maximum likelihood estimation (MLE), as stated in the equation below, the objective is to maximize the likelihood of the data being generated by the generative model. In other words, we want to find the value of the parameters \(\theta\) so that the above model best explains the acoustic features (e.g., spectrogram) that we observe.</p>

\[\begin{align*}
&amp;\arg\max_\theta \prod_{n=1}^N \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right)\\
=&amp;\arg\max_\theta \sum_{n=1}^N \log \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right).
\end{align*}\]

<p>For ease of notation, for the rest of this section, let’s ignore the conditioning on \({\bf y}^{(n)}\) (or \({\bf p}^{(n)}\)). The difficulty in evaluating the above log-likelihood lies in the need to marginalize over all potential values of \({\bf z}^{(n)}\). This formulation falls right into the discussion of the previous two posts:  <a href="/blog/2018/variational-inference-I-variational-lower-bound/">variational lower bound</a> and <a href="/blog/2018/variational-inference-II-expectation-maximization/">expectation maximization</a>, which provide an iterative algorithm to approach the solution</p>

\[\begin{align}
\theta^{[i+1]} = \arg\max_{\theta} \sum_{n=1}^N \int \color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)}\log \mathbb{P}\left({\bf x}^{(n)}, {\bf z}^{(n)};\theta\right)d z^{(n)}.
\end{align}\]

<p>Most of the computation complexity in the above equation lies in finding the posterior probability of the latent state given the observed \(\color{red}{\mathbb{P}\left({\bf z}^{(n)}\| {\bf x}^{(n)};\theta\right)}\). To elaborate on how the posterior probability is computed, let’s expand the acoustic model part in the previous figure as below, which is essentially a hidden-Markov chain.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/asr/HMM2-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/asr/HMM2-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/asr/HMM2-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/asr/HMM2.png" />

  </picture>

</figure>

    </div>
</div>

<p>The inference problem (finding the posterior of the latent given the observed) in a hidden Markov chain can be solved by a forward-backward algorithm. The algorithm manifests itself as BCJR algorithm in convolutional code bit-level MAP decoding and <a href="/blog/2018/kalman-filter-and-particle-filter/">Kalman filtering</a> in linear dynamic system.</p>

\[\begin{align}
\text{Forward path: }&amp;\text{calculate }\mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\text{ from }\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to t};\theta\right) = \sum_{z_{t-1}} \mathbb{P}(z_{t}|z_{t-1};\theta)\mathbb{P}(x_{t}|z_{t};\theta)\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right)  \notag \\
\text{Backward path: }&amp;\text{calculate }\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right)\text{ from }\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
&amp;\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) = \sum_{z_{t+1}} \mathbb{P}(z_{t+1}|z_{t};\theta)\mathbb{P}(x_{t+1}|z_{t+1};\theta)\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
\text{Combined: }&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t| {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) / \sum_{z_t}\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right)
\end{align}\]

<h2 id="circular-dependency-between-segmentation-and-recognition">Circular dependency between segmentation and recognition</h2>
<p>The expectation-maximization formulation for likelihood maximization reveals a fundamental circular dependency between segmentation and recognition.</p>

<p>Here <strong>segmentation</strong> refers to the alignment of sub-phoneme states of \({\bf y}\) and the acoustic feature observations \({\bf x}\), encoded in the hidden state sequence \({\bf z}\), and <strong>recognition</strong> refers to the classification of sub-phoneme hidden state sequence \({\bf z}\) for the corresponds acoustic feature observations \({\bf x}\).</p>

<p>The two equations below make the circular dependency precise:</p>

\[\begin{align*}
\theta^{[i]} &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update soft-alignment}\\\text{based on recognition}}}} \mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)\text{ using Equation (2)}\\
\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right) &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update recognition}\\\text{based on soft-alignment}}}}
\theta^{[i+1]}\text{ using Equation (1)}
\end{align*}\]

<p>It is easy to argue that to have an accurate alignment, we need accurate recognition, and to train an accurate recognition, we have to rely on accurate alignment/segmentation.</p>

<p>In a convention ASR system, to bootstrap the training procedure, we have to start with a dataset that has human curated phoneme boundary/segmentation. Once the system is capacitated with reasonable recognition/inference, it is no longer confined with human aligned dataset and a much larger dataset can be used with just waveform and the corresponding phoneme transcription. Eventually, after the system is able to deliver robust segmentation, we can make hard decision on the alignment, and only focus on improving the recognition performance with potentially a different system that has a much larger capacity, e.g., a DNN replacing the GMM model.</p>

<h2 id="decoding">Decoding</h2>
<p>In the decoding stage, we try to find the word/sentence with the maximum a posterior (MAP) probability given the observed data</p>

\[\begin{align*}
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{=}&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) \\
=&amp;\arg\max_{\bf y} \underbrace{\mathbb{P}({\bf x}|{\bf p};\theta)}_{\text{acoustic model}}\times\underbrace{\mathbb{P}({\bf p}|{\bf y})}_{\text{lexion}}\times\underbrace{\mathbb{P}({\bf y})}_{\text{language model}}
\end{align*}\]

<p>The lexicon and language model together construct a state transition diagram, which we unrolled in time to form a decoding trellis. For each transcription hypothesis, a proper MAP decoder would sum across all the paths in the trellis that corresponds to the transcription, which is computationally prohibitive.</p>

<p>One simplification one can make is to find the most probable path by running the Viterbi algorithm. However, even for Viterbi algorithm, the complexity is still too high for practical deployment due to the large state space and potentially large number of time steps.</p>

<p>To further reduce the computation complexity, the conventional system resorts to the beam-search algorithm – basically a breath-first-search algorithm on the trellis that maintain only a limited number of candidates. The beam-search algorithm is often run on a weighted finite state transducer that captures the concatenation of language model and lexicon.</p>

<h2 id="discrepancy-between-mle-training-and-map-decoding">Discrepancy between MLE training and MAP decoding</h2>

<p>At first glance into the MAP decoding equation, it may appear that the MLE based training is well-aligned with the decoding process: maximizing the posterior probability of the transcription \({\bf y}\) given the acoustic feature \({\bf x}\) is equivalent to maximizing the likelihood of the observation \({\bf x}\). The argument being that the probability of the observation \(\mathbb{P}(x;\theta)\) is anyway a constant dictated by the natural of people’s speech, not something we can control. But is it true?</p>

\[\begin{align*}
&amp;\text{inference time:}\\
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*}\]

<p>It turns out there is a subtle difference between inference time (MAP decoding) and training time (MLE parameter update) that render the above statement wrong.</p>

\[\begin{align*}
&amp;\text{training time:}\\
&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{\not=}&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*}\]

<p>As is evident by comparing the above two equations, when we try to update parameter \(\theta\) to maximize directly the posterior probability of the transcription \({\bf y}\) given the acoustic feature \({\bf x}\), we can no longer ignore the term \(\mathbb{P}({\bf x};\theta)\). The key is to realize that we model the speech as a generative model, where <strong>the probability of observing a certain acoustic features \({\bf x}\) is not dictated by the nature, but rather the generative model that we assume</strong>. By updating the parameter \(\theta\) that best increase the likelihood, we inevitably change \(\mathbb{P}({\bf x};\theta)\) too, and thus there is no guarantee that the posterior probability is increased. \(\mathbb{P}({\bf x};\theta)\) is calculated by marginalizing over all potential transcriptions: \(\mathbb{P}({\bf x};\theta)=\sum_{\bf y}\mathbb{P}({\bf x}\|{\bf y};\theta)\).</p>

<p>To elaborate, in MLE, we try to maximize \(\color{red}{\mathbb{P}({\bf y}\|{\bf x};\theta)}\) with respect to \(\theta\), we may very well also increased the likelihood for competing transcription sequences \(\color{blue}{\mathbb{P}({\bf x}\|{\bf \tilde{y}};\theta)}\), potentially resulting in decreased posterior probability \(\mathbb{P}({\bf y}\|{\bf x};\theta)\).</p>

\[\begin{align*}
&amp;\mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp; \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp; \frac{\color{red}{\mathbb{P}({\bf x}|{\bf y};\theta)}\mathbb{P}({\bf y})}{\sum_{\bf \tilde{y}}\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}\mathbb{P}({\bf \tilde{y}})}
\end{align*}\]

<p>Fundamentally, <strong>the misalignment is rooted from the fact that we are using a generative model for discriminative tasks</strong>. We end here by noting that several sequence discriminative training methods were proposed for better discrimination in the literature.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[call me an archeologist]]></summary></entry><entry><title type="html">Comparison of end-to-end ASR models</title><link href="https://yyang768osu.github.io/blog/2018/comparison-of-end-2-end-asr-models/" rel="alternate" type="text/html" title="Comparison of end-to-end ASR models" /><published>2018-11-13T00:00:00+00:00</published><updated>2018-11-13T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2018/comparison-of-end-2-end-asr-models</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2018/comparison-of-end-2-end-asr-models/"><![CDATA[<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/asr/end2end-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/asr/end2end-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/asr/end2end-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/asr/end2end.png" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/asr/transcription_model-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/asr/transcription_model-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/asr/transcription_model-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/asr/transcription_model.png" />

  </picture>

</figure>

    </div>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[how CTC, RNN-transducer, and Attention factor probabilities]]></summary></entry><entry><title type="html">Gumbel max and Gumbel softmax</title><link href="https://yyang768osu.github.io/blog/2018/gumbel-max-and-gumbel-softmax/" rel="alternate" type="text/html" title="Gumbel max and Gumbel softmax" /><published>2018-11-05T00:00:00+00:00</published><updated>2018-11-05T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2018/gumbel-max-and-gumbel-softmax</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2018/gumbel-max-and-gumbel-softmax/"><![CDATA[<p>Before we talk about Gumbel distribution, let’s refresh our knowledge on exponential distribution. It is well-known that the exponential distribution is min-stable: the min of \(n\) I.I.D. exponential random variables  \(X_i\sim \text{Exp}(\lambda_i), i=1,2,\ldots, n\) is also exponentially distributed with decay rate \(\sum_{1\leq i\leq n} \lambda_i\), as can be seen from the equation below.</p>

\[\begin{align*}
P(\min_{1\leq i \leq n} X_i &gt; x) &amp;= \prod_{1\leq i\leq n} P(X_i&gt;x) = e^{-\sum_{1\leq i\leq n}\lambda_i x}
\end{align*}\]

<p>A lesser known property is that the arg-min of exponential variables is a multinomial distribution with event probabilities $$\left(\frac{\lambda_1}{\sum_{i}\lambda_i}, \ldots, \frac{\lambda_n}{\sum_{i}\lambda_i}\right)$:</p>

\[\begin{align*}
&amp;P(\arg\min_{1\leq i\leq n}X_i =k)\\
=&amp;\int_0^\infty P(\arg\min_{1\leq i\leq n} X_i = x | X_k=x)f(X_k = x)dx\\
=&amp;\int_0^\infty \prod_{i=1\to n, i\not=k} e^{-\lambda_i x} \lambda_k e^{-\lambda_k x} dx\\
=&amp;\frac{\lambda_k}{\sum_{i=1\to n} \lambda_i}.
\end{align*}\]

<p>As we will see shortly, this property directly leads to the Gumbel max trick.</p>

<h2 id="gumbel-max-trick">Gumbel max trick</h2>

<p>Assume that we are given a multinomial (a.k.a. categorical) distribution with unnormalized event probabilities \(\lambda_i, i=1\to n\) (i.e., \(\sum_{i=1}^n\lambda_i\not=1\)), the above property provides us a way to sample from the distribution without the need for normalization:</p>
<ol>
  <li>draw \(n\) samples from an exponential distributions with decay rate of \(1\)</li>
  <li>scale the value of these \(n\) samples with \(1/\lambda_i\) for \(i=1\to n\).</li>
  <li>take the index of the minimum of the scaled samples</li>
</ol>

<p>To be more precise, we are utilizing the fact that</p>

\[\begin{align*}
&amp;\arg\min_{1\leq i\leq n} \left(\frac{1}{\lambda_i}s_i\right) \sim \text{Multinomial}\left(\frac{\lambda_1}{\sum_{i}\lambda_i}, \ldots, \frac{\lambda_n}{\sum_{i}\lambda_i}\right)\\
&amp;\text{where } s_i\sim \text{Exp}(1), \forall i.
\end{align*}\]

<p>For the case of soft-max operation, we have direct access to the log of the unnormalized probabilities \(\alpha_i=\log \lambda_i\) (multinomial logit), instead of the unnormalized probabilities itself. In this case, we can modify the above equation as below</p>

\[\begin{align*}
&amp;\arg\min_{1\leq i\leq n} \left(\frac{1}{e^{\alpha_i}}s_i\right) \sim \text{Multinomial}\left(\frac{e^{\alpha_1}}{\sum_{i}e^{\alpha_i}}, \ldots, \frac{e^{\alpha_n}}{\sum_{i}e^{\alpha_i}}\right)\\
&amp;\text{where } s_i\sim \text{Exp}(1), \forall i.
\end{align*}\]

<p>One observation is that the left hand side of the above equation is invariant to any linear transform. The Gumbel-max trick is obtained by taking \(-\log(\cdot)\) operation to the right-hand-side, in which case \(-\log(\text{Exp}(1))\) is a standard Gumbel distribution, leading to the equation below</p>

\[\begin{align*}
&amp;\arg\max_{1\leq i\leq n} \left(\alpha_i+g_i\right) \sim \text{Multinomial}\left(\frac{e^{\alpha_1}}{\sum_{i}e^{\alpha_i}}, \ldots, \frac{e^{\alpha_n}}{\sum_{i}e^{\alpha_i}}\right)\\
&amp;\text{where } g_i\sim \text{Gumbel}(\text{location}=0, \text{scale}=1), \forall i.
\end{align*}\]

<p>This provides us a way to obtain samples directly from the logits without going through the exponentiate-and-normalization step</p>

<ol>
  <li>draw \(n\) samples from a standard Gumbel distributions with location of \(0\) and scale of \(1\).</li>
  <li>add the values of the \(n\) samples to the logits.</li>
  <li>take the index of the minimum of the \(n\) summations.</li>
</ol>

<p>Essentially, <em>the Gumbel max trick converts the sampling operation from a categorical/multinomial distribution into an argmax operation</em>. The sampling process can be expedited if we pre-calculate and store a stream of Gumbel samples.</p>

<h2 id="gumbel-softmax">Gumbel softmax</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/gumbel/gumbel-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/gumbel/gumbel-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/gumbel/gumbel-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/gumbel/gumbel.png" />

  </picture>

</figure>

    </div>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[sampling of softmax == max of (logit + Gumbel noise)]]></summary></entry></feed>